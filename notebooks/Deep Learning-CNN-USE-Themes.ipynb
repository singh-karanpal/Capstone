{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model - CNN Multi-Label Text Classification\n",
    "### Universal Sentence Encoder with Sub Themes (Right Now Ran for Main Themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling1D, MaxPool1D, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, hamming_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Q1 = pd.read_excel('../data/interim/X_train_Q1_clean.xlsx')\n",
    "X_valid_Q1 = pd.read_excel('../data/interim/X_valid_Q1_clean.xlsx')\n",
    "\n",
    "y_train_Q1 = pd.read_excel('../data/interim/y_train_Q1.xlsx')\n",
    "y_valid_Q1 = pd.read_excel('../data/interim/y_valid_Q1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Unified Dataframe for CNN Ready Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_train_Q1, y_train_Q1.iloc[:,0:12]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>CPD</th>\n",
       "      <th>CB</th>\n",
       "      <th>EWC</th>\n",
       "      <th>Exec</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SP</th>\n",
       "      <th>RE</th>\n",
       "      <th>Sup</th>\n",
       "      <th>SW</th>\n",
       "      <th>TEPE</th>\n",
       "      <th>VMG</th>\n",
       "      <th>OTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to be real about diversity, you need to create...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep the building warmer and provide warm wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better communication from the top down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be beneficial if Management did not m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more education applicable to my job</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  CPD  CB  EWC  Exec  FEW  \\\n",
       "0  to be real about diversity, you need to create...    0   0    1     0    0   \n",
       "1  Keep the building warmer and provide warm wate...    0   0    0     0    0   \n",
       "2             better communication from the top down    0   0    0     1    0   \n",
       "3  It would be beneficial if Management did not m...    0   0    0     0    0   \n",
       "4                more education applicable to my job    1   0    0     0    0   \n",
       "\n",
       "   SP  RE  Sup  SW  TEPE  VMG  OTH  \n",
       "0   0   0    0   0     0    0    0  \n",
       "1   0   0    0   0     1    0    0  \n",
       "2   0   0    0   0     0    0    0  \n",
       "3   0   1    0   0     0    0    0  \n",
       "4   0   0    0   0     0    0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in data_df['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "data_df['preprocessed_comments']=preprocessed_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>CPD</th>\n",
       "      <th>CB</th>\n",
       "      <th>EWC</th>\n",
       "      <th>Exec</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SP</th>\n",
       "      <th>RE</th>\n",
       "      <th>Sup</th>\n",
       "      <th>SW</th>\n",
       "      <th>TEPE</th>\n",
       "      <th>VMG</th>\n",
       "      <th>OTH</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to be real about diversity, you need to create...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>real diversity you need create seats table mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep the building warmer and provide warm wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>keep building warmer provide warm water bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better communication from the top down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>better communication top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be beneficial if Management did not m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>would beneficial management not micro manage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more education applicable to my job</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>education applicable job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  CPD  CB  EWC  Exec  FEW  \\\n",
       "0  to be real about diversity, you need to create...    0   0    1     0    0   \n",
       "1  Keep the building warmer and provide warm wate...    0   0    0     0    0   \n",
       "2             better communication from the top down    0   0    0     1    0   \n",
       "3  It would be beneficial if Management did not m...    0   0    0     0    0   \n",
       "4                more education applicable to my job    1   0    0     0    0   \n",
       "\n",
       "   SP  RE  Sup  SW  TEPE  VMG  OTH  \\\n",
       "0   0   0    0   0     0    0    0   \n",
       "1   0   0    0   0     1    0    0   \n",
       "2   0   0    0   0     0    0    0   \n",
       "3   0   1    0   0     0    0    0   \n",
       "4   0   0    0   0     0    0    0   \n",
       "\n",
       "                               preprocessed_comments  \n",
       "0  real diversity you need create seats table mea...  \n",
       "1   keep building warmer provide warm water bathroom  \n",
       "2                           better communication top  \n",
       "3       would beneficial management not micro manage  \n",
       "4                           education applicable job  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[['preprocessed_comments']]\n",
    "y_train = data_df.drop(['Comment', 'preprocessed_comments'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([X_valid_Q1, y_valid_Q1.iloc[:,:12]], axis = 1)\n",
    "\n",
    "# pre-processing test data\n",
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df_valid['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "    \n",
    "    \n",
    "df_valid['preprocessed_comments']=preprocessed_synopsis\n",
    "\n",
    "X_valid = df_valid[['preprocessed_comments']]\n",
    "y_valid = df_valid.drop(columns=['Comment', 'preprocessed_comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max length of sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(x):\n",
    "    a=x.split()\n",
    "    return len(a)\n",
    "\n",
    "max_len = max(data_df['Comment'].apply(max_len))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=Tokenizer()\n",
    "vect.fit_on_texts(X_train['preprocessed_comments'])\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universal Sentence Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(X_train['preprocessed_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "\n",
    "max_features = embedding_matrix.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                        trainable=False, input_length=maxlen))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(padded_docs_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Simple Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 150\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(max_features, input_shape=(512,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(max_features, input_shape=(512,)))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(n_class))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedding_matrix, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([X_valid_Q1, y_valid_Q1.iloc[:,:12]], axis = 1)\n",
    "\n",
    "# pre-processing test data\n",
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df_valid['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "    \n",
    "    \n",
    "df_valid['preprocessed_comments']=preprocessed_synopsis\n",
    "\n",
    "# creating X and Y\n",
    "X_valid = df_valid[['preprocessed_comments']]\n",
    "y_valid = df_valid.drop(columns=['Comment', 'preprocessed_comments'])\n",
    "\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# creating embedding for validation\n",
    "embeddings_valid = embed(X_valid['preprocessed_comments'])\n",
    "embedding_matrix_valid = np.array(embeddings_valid)\n",
    "\n",
    "score = model.evaluate(embedding_matrix_valid,y_valid)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(embedding_matrix_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#predictions=model.predict([padded_docs_test])\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid, pred, average='micro')\n",
    "    recall = recall_score(y_valid, pred, average='micro')\n",
    "    f1 = f1_score(y_valid, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[512])\n",
    "    layer = Embedding(max_features,50,input_length=512)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(12,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedding_matrix, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Testing ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score on Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([X_valid_Q1, y_valid_Q1.iloc[:,:12]], axis = 1)\n",
    "\n",
    "# pre-processing test data\n",
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df_valid['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "    \n",
    "    \n",
    "df_valid['preprocessed_comments']=preprocessed_synopsis\n",
    "\n",
    "# creating X and Y\n",
    "\n",
    "X_valid = df_valid[['preprocessed_comments']]\n",
    "y_valid = df_valid.drop(columns=['Comment', 'preprocessed_comments'])\n",
    "\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# creating padded dataset for x_valid\n",
    "encoded_docs_valid = vect.texts_to_sequences(X_valid['preprocessed_comments'])\n",
    "padded_docs_valid = pad_sequences(encoded_docs_valid, maxlen=max_len, padding='post')\n",
    "print(padded_docs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(padded_docs_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/cnn_use_sub_themes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "pred = model.predict(padded_docs_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions=model.predict([padded_docs_test])\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid, pred, average='micro')\n",
    "    recall = recall_score(y_valid, pred, average='micro')\n",
    "    f1 = f1_score(y_valid, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Way of Running USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_array = np.asarray(X_train['preprocessed_comments'])\n",
    "y_train_array = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_array = np.asarray(X_valid['preprocessed_comments'])\n",
    "y_valid_array = np.asarray(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_features = embedding_matrix.shape[0]\n",
    "#maxlen = max_len\n",
    "batch_size = 150\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[512], input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 512), dtype=float32, numpy=\n",
       "array([[-0.0366986 , -0.07417478,  0.03231421, ...,  0.01621996,\n",
       "         0.00728293,  0.00265465],\n",
       "       [ 0.05218185, -0.01258107, -0.04657765, ...,  0.05539098,\n",
       "         0.05895474, -0.01712018],\n",
       "       [-0.01777639, -0.04568463,  0.00473103, ..., -0.00073466,\n",
       "        -0.08261821,  0.05859691],\n",
       "       [ 0.04762139, -0.06028655,  0.06576026, ...,  0.04358777,\n",
       "        -0.08644933, -0.02078426]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_layer(X_train_array[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_3 (KerasLayer)   (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 60)                30780     \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 500)               30500     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 256,980,616\n",
      "Trainable params: 256,980,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(60, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(200, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(100, activation = 'sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(12, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10376 samples, validate on 2594 samples\n",
      "Epoch 1/50\n",
      "10376/10376 [==============================] - 42s 4ms/sample - loss: 0.5325 - accuracy: 0.7184 - val_loss: 0.3565 - val_accuracy: 0.8838\n",
      "Epoch 2/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3536 - accuracy: 0.8830 - val_loss: 0.3498 - val_accuracy: 0.8838\n",
      "Epoch 3/50\n",
      "10376/10376 [==============================] - 39s 4ms/sample - loss: 0.3523 - accuracy: 0.8830 - val_loss: 0.3492 - val_accuracy: 0.8838\n",
      "Epoch 4/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3520 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 5/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3520 - accuracy: 0.8830 - val_loss: 0.3491 - val_accuracy: 0.8838\n",
      "Epoch 6/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3489 - val_accuracy: 0.8838\n",
      "Epoch 7/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3489 - val_accuracy: 0.8838\n",
      "Epoch 8/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 9/50\n",
      "10376/10376 [==============================] - 36s 4ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3489 - val_accuracy: 0.8838\n",
      "Epoch 10/50\n",
      "10376/10376 [==============================] - 36s 4ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 11/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 12/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 13/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 14/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3491 - val_accuracy: 0.8838\n",
      "Epoch 15/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 16/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3518 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 17/50\n",
      "10376/10376 [==============================] - 38s 4ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 18/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3490 - val_accuracy: 0.8838\n",
      "Epoch 19/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3517 - accuracy: 0.8830 - val_loss: 0.3488 - val_accuracy: 0.8838\n",
      "Epoch 20/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3514 - accuracy: 0.8830 - val_loss: 0.3477 - val_accuracy: 0.8838\n",
      "Epoch 21/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3477 - accuracy: 0.8830 - val_loss: 0.3401 - val_accuracy: 0.8838\n",
      "Epoch 22/50\n",
      "10376/10376 [==============================] - 38s 4ms/sample - loss: 0.3380 - accuracy: 0.8830 - val_loss: 0.3306 - val_accuracy: 0.8838\n",
      "Epoch 23/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.3223 - accuracy: 0.8836 - val_loss: 0.3143 - val_accuracy: 0.8937\n",
      "Epoch 24/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.3032 - accuracy: 0.8937 - val_loss: 0.3017 - val_accuracy: 0.8949\n",
      "Epoch 25/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2872 - accuracy: 0.8983 - val_loss: 0.2949 - val_accuracy: 0.8995\n",
      "Epoch 26/50\n",
      "10376/10376 [==============================] - 38s 4ms/sample - loss: 0.2754 - accuracy: 0.9041 - val_loss: 0.2880 - val_accuracy: 0.9016\n",
      "Epoch 27/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2655 - accuracy: 0.9061 - val_loss: 0.2865 - val_accuracy: 0.9020\n",
      "Epoch 28/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2584 - accuracy: 0.9071 - val_loss: 0.2811 - val_accuracy: 0.9021\n",
      "Epoch 29/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2506 - accuracy: 0.9113 - val_loss: 0.2796 - val_accuracy: 0.9056\n",
      "Epoch 30/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2440 - accuracy: 0.9140 - val_loss: 0.2805 - val_accuracy: 0.9047\n",
      "Epoch 31/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2394 - accuracy: 0.9150 - val_loss: 0.2778 - val_accuracy: 0.9058\n",
      "Epoch 32/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2342 - accuracy: 0.9156 - val_loss: 0.2771 - val_accuracy: 0.9056\n",
      "Epoch 33/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2295 - accuracy: 0.9196 - val_loss: 0.2764 - val_accuracy: 0.9052\n",
      "Epoch 34/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2259 - accuracy: 0.9255 - val_loss: 0.2732 - val_accuracy: 0.9074\n",
      "Epoch 35/50\n",
      "10376/10376 [==============================] - 36s 4ms/sample - loss: 0.2215 - accuracy: 0.9288 - val_loss: 0.2733 - val_accuracy: 0.9062\n",
      "Epoch 36/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.2174 - accuracy: 0.9306 - val_loss: 0.2740 - val_accuracy: 0.9074\n",
      "Epoch 37/50\n",
      "10376/10376 [==============================] - 38s 4ms/sample - loss: 0.2144 - accuracy: 0.9314 - val_loss: 0.2742 - val_accuracy: 0.9068\n",
      "Epoch 38/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.2112 - accuracy: 0.9350 - val_loss: 0.2729 - val_accuracy: 0.9065\n",
      "Epoch 39/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2087 - accuracy: 0.9357 - val_loss: 0.2753 - val_accuracy: 0.9062\n",
      "Epoch 40/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2059 - accuracy: 0.9367 - val_loss: 0.2766 - val_accuracy: 0.9052\n",
      "Epoch 41/50\n",
      "10376/10376 [==============================] - 38s 4ms/sample - loss: 0.2036 - accuracy: 0.9375 - val_loss: 0.2735 - val_accuracy: 0.9067\n",
      "Epoch 42/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.2014 - accuracy: 0.9375 - val_loss: 0.2774 - val_accuracy: 0.9045\n",
      "Epoch 43/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.1997 - accuracy: 0.9377 - val_loss: 0.2801 - val_accuracy: 0.9040\n",
      "Epoch 44/50\n",
      "10376/10376 [==============================] - 37s 4ms/sample - loss: 0.1975 - accuracy: 0.9381 - val_loss: 0.2775 - val_accuracy: 0.9069\n",
      "Epoch 45/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1956 - accuracy: 0.9383 - val_loss: 0.2801 - val_accuracy: 0.9045\n",
      "Epoch 46/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1942 - accuracy: 0.9380 - val_loss: 0.2721 - val_accuracy: 0.9088\n",
      "Epoch 47/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1923 - accuracy: 0.9382 - val_loss: 0.2772 - val_accuracy: 0.9058\n",
      "Epoch 48/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1903 - accuracy: 0.9390 - val_loss: 0.2806 - val_accuracy: 0.9066\n",
      "Epoch 49/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1890 - accuracy: 0.9415 - val_loss: 0.2740 - val_accuracy: 0.9089\n",
      "Epoch 50/50\n",
      "10376/10376 [==============================] - 36s 3ms/sample - loss: 0.1872 - accuracy: 0.9444 - val_loss: 0.2765 - val_accuracy: 0.9084\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_array,\n",
    "                    y_train_array,\n",
    "                    epochs=50,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_valid_array, y_valid_array),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 1s 290us/sample\n",
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2905, Recall: 0.6976, F1-measure: 0.4102\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4328, Recall: 0.5558, F1-measure: 0.4867\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5563, Recall: 0.4613, F1-measure: 0.5044\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6053, Recall: 0.4281, F1-measure: 0.5015\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7196, Recall: 0.3469, F1-measure: 0.4681\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7335, Recall: 0.3195, F1-measure: 0.4451\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7729, Recall: 0.2869, F1-measure: 0.4185\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9182, Recall: 0.1985, F1-measure: 0.3264\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9529, Recall: 0.1567, F1-measure: 0.2692\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_valid_array, batch_size=512, verbose=1)\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid_array, pred, average='micro')\n",
    "    recall = recall_score(y_valid_array, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_array, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[20], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_6 (KerasLayer)   (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 60)                1260      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 500)               30500     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 553,292\n",
      "Trainable params: 553,292\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(60, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(500, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(200, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(100, activation = 'sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(12, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10376 samples, validate on 2594 samples\n",
      "Epoch 1/300\n",
      "10376/10376 [==============================] - 1s 119us/sample - loss: 0.4100 - accuracy: 0.8458 - val_loss: 0.3442 - val_accuracy: 0.8838\n",
      "Epoch 2/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.3357 - accuracy: 0.8849 - val_loss: 0.3213 - val_accuracy: 0.8925\n",
      "Epoch 3/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.3173 - accuracy: 0.8923 - val_loss: 0.3064 - val_accuracy: 0.8934\n",
      "Epoch 4/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.3027 - accuracy: 0.8934 - val_loss: 0.2940 - val_accuracy: 0.8937\n",
      "Epoch 5/300\n",
      "10376/10376 [==============================] - 1s 53us/sample - loss: 0.2887 - accuracy: 0.8941 - val_loss: 0.2810 - val_accuracy: 0.8947\n",
      "Epoch 6/300\n",
      "10376/10376 [==============================] - 1s 58us/sample - loss: 0.2738 - accuracy: 0.8977 - val_loss: 0.2679 - val_accuracy: 0.9012\n",
      "Epoch 7/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.2612 - accuracy: 0.9038 - val_loss: 0.2576 - val_accuracy: 0.9062\n",
      "Epoch 8/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.2495 - accuracy: 0.9087 - val_loss: 0.2494 - val_accuracy: 0.9091\n",
      "Epoch 9/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.2397 - accuracy: 0.9122 - val_loss: 0.2433 - val_accuracy: 0.9117\n",
      "Epoch 10/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.2301 - accuracy: 0.9164 - val_loss: 0.2374 - val_accuracy: 0.9129\n",
      "Epoch 11/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.2210 - accuracy: 0.9203 - val_loss: 0.2338 - val_accuracy: 0.9143\n",
      "Epoch 12/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.2140 - accuracy: 0.9238 - val_loss: 0.2290 - val_accuracy: 0.9165\n",
      "Epoch 13/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.2064 - accuracy: 0.9271 - val_loss: 0.2255 - val_accuracy: 0.9171\n",
      "Epoch 14/300\n",
      "10376/10376 [==============================] - 1s 55us/sample - loss: 0.2000 - accuracy: 0.9294 - val_loss: 0.2239 - val_accuracy: 0.9180\n",
      "Epoch 15/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.1938 - accuracy: 0.9318 - val_loss: 0.2212 - val_accuracy: 0.9186\n",
      "Epoch 16/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.1881 - accuracy: 0.9346 - val_loss: 0.2213 - val_accuracy: 0.9181\n",
      "Epoch 17/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.1828 - accuracy: 0.9359 - val_loss: 0.2213 - val_accuracy: 0.9173\n",
      "Epoch 18/300\n",
      "10376/10376 [==============================] - 1s 63us/sample - loss: 0.1780 - accuracy: 0.9383 - val_loss: 0.2196 - val_accuracy: 0.9191\n",
      "Epoch 19/300\n",
      "10376/10376 [==============================] - 1s 59us/sample - loss: 0.1734 - accuracy: 0.9400 - val_loss: 0.2183 - val_accuracy: 0.9187\n",
      "Epoch 20/300\n",
      "10376/10376 [==============================] - 0s 47us/sample - loss: 0.1685 - accuracy: 0.9419 - val_loss: 0.2207 - val_accuracy: 0.9184\n",
      "Epoch 21/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1642 - accuracy: 0.9435 - val_loss: 0.2205 - val_accuracy: 0.9187\n",
      "Epoch 22/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.1601 - accuracy: 0.9456 - val_loss: 0.2207 - val_accuracy: 0.9190\n",
      "Epoch 23/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1558 - accuracy: 0.9472 - val_loss: 0.2214 - val_accuracy: 0.9190\n",
      "Epoch 24/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1517 - accuracy: 0.9491 - val_loss: 0.2212 - val_accuracy: 0.9201\n",
      "Epoch 25/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1479 - accuracy: 0.9504 - val_loss: 0.2243 - val_accuracy: 0.9180\n",
      "Epoch 26/300\n",
      "10376/10376 [==============================] - 1s 51us/sample - loss: 0.1442 - accuracy: 0.9521 - val_loss: 0.2259 - val_accuracy: 0.9190\n",
      "Epoch 27/300\n",
      "10376/10376 [==============================] - 1s 51us/sample - loss: 0.1404 - accuracy: 0.9537 - val_loss: 0.2270 - val_accuracy: 0.9184\n",
      "Epoch 28/300\n",
      "10376/10376 [==============================] - 1s 52us/sample - loss: 0.1373 - accuracy: 0.9547 - val_loss: 0.2273 - val_accuracy: 0.9185\n",
      "Epoch 29/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1337 - accuracy: 0.9560 - val_loss: 0.2295 - val_accuracy: 0.9187\n",
      "Epoch 30/300\n",
      "10376/10376 [==============================] - 0s 36us/sample - loss: 0.1298 - accuracy: 0.9578 - val_loss: 0.2314 - val_accuracy: 0.9180\n",
      "Epoch 31/300\n",
      "10376/10376 [==============================] - 0s 36us/sample - loss: 0.1269 - accuracy: 0.9589 - val_loss: 0.2328 - val_accuracy: 0.9183\n",
      "Epoch 32/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.1235 - accuracy: 0.9602 - val_loss: 0.2357 - val_accuracy: 0.9172\n",
      "Epoch 33/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1205 - accuracy: 0.9614 - val_loss: 0.2380 - val_accuracy: 0.9166\n",
      "Epoch 34/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1175 - accuracy: 0.9626 - val_loss: 0.2405 - val_accuracy: 0.9167\n",
      "Epoch 35/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1142 - accuracy: 0.9639 - val_loss: 0.2421 - val_accuracy: 0.9168\n",
      "Epoch 36/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.1119 - accuracy: 0.9648 - val_loss: 0.2458 - val_accuracy: 0.9163\n",
      "Epoch 37/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1097 - accuracy: 0.9653 - val_loss: 0.2473 - val_accuracy: 0.9164\n",
      "Epoch 38/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1068 - accuracy: 0.9668 - val_loss: 0.2513 - val_accuracy: 0.9155\n",
      "Epoch 39/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1042 - accuracy: 0.9672 - val_loss: 0.2546 - val_accuracy: 0.9150\n",
      "Epoch 40/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.1010 - accuracy: 0.9687 - val_loss: 0.2565 - val_accuracy: 0.9155\n",
      "Epoch 41/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0989 - accuracy: 0.9693 - val_loss: 0.2609 - val_accuracy: 0.9148\n",
      "Epoch 42/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0959 - accuracy: 0.9708 - val_loss: 0.2645 - val_accuracy: 0.9135\n",
      "Epoch 43/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0942 - accuracy: 0.9711 - val_loss: 0.2673 - val_accuracy: 0.9149\n",
      "Epoch 44/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0915 - accuracy: 0.9721 - val_loss: 0.2690 - val_accuracy: 0.9144\n",
      "Epoch 45/300\n",
      "10376/10376 [==============================] - 0s 37us/sample - loss: 0.0894 - accuracy: 0.9727 - val_loss: 0.2750 - val_accuracy: 0.9127\n",
      "Epoch 46/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0880 - accuracy: 0.9735 - val_loss: 0.2757 - val_accuracy: 0.9118\n",
      "Epoch 47/300\n",
      "10376/10376 [==============================] - 0s 37us/sample - loss: 0.0863 - accuracy: 0.9738 - val_loss: 0.2791 - val_accuracy: 0.9123\n",
      "Epoch 48/300\n",
      "10376/10376 [==============================] - 0s 37us/sample - loss: 0.0831 - accuracy: 0.9752 - val_loss: 0.2825 - val_accuracy: 0.9116\n",
      "Epoch 49/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0810 - accuracy: 0.9757 - val_loss: 0.2863 - val_accuracy: 0.9113\n",
      "Epoch 50/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0792 - accuracy: 0.9766 - val_loss: 0.2908 - val_accuracy: 0.9104\n",
      "Epoch 51/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0778 - accuracy: 0.9775 - val_loss: 0.2943 - val_accuracy: 0.9102\n",
      "Epoch 52/300\n",
      "10376/10376 [==============================] - 0s 48us/sample - loss: 0.0758 - accuracy: 0.9778 - val_loss: 0.2979 - val_accuracy: 0.9102\n",
      "Epoch 53/300\n",
      "10376/10376 [==============================] - 1s 50us/sample - loss: 0.0746 - accuracy: 0.9780 - val_loss: 0.2988 - val_accuracy: 0.9099\n",
      "Epoch 54/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0720 - accuracy: 0.9790 - val_loss: 0.3020 - val_accuracy: 0.9099\n",
      "Epoch 55/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0701 - accuracy: 0.9800 - val_loss: 0.3054 - val_accuracy: 0.9088\n",
      "Epoch 56/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0691 - accuracy: 0.9798 - val_loss: 0.3119 - val_accuracy: 0.9112\n",
      "Epoch 57/300\n",
      "10376/10376 [==============================] - 0s 37us/sample - loss: 0.0673 - accuracy: 0.9804 - val_loss: 0.3145 - val_accuracy: 0.9091\n",
      "Epoch 58/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0666 - accuracy: 0.9806 - val_loss: 0.3177 - val_accuracy: 0.9093\n",
      "Epoch 59/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0644 - accuracy: 0.9813 - val_loss: 0.3233 - val_accuracy: 0.9094\n",
      "Epoch 60/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0627 - accuracy: 0.9821 - val_loss: 0.3232 - val_accuracy: 0.9089\n",
      "Epoch 61/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0618 - accuracy: 0.9822 - val_loss: 0.3273 - val_accuracy: 0.9081\n",
      "Epoch 62/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.3340 - val_accuracy: 0.9082\n",
      "Epoch 63/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0589 - accuracy: 0.9837 - val_loss: 0.3334 - val_accuracy: 0.9073\n",
      "Epoch 64/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0575 - accuracy: 0.9836 - val_loss: 0.3387 - val_accuracy: 0.9071\n",
      "Epoch 65/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0559 - accuracy: 0.9843 - val_loss: 0.3404 - val_accuracy: 0.9063\n",
      "Epoch 66/300\n",
      "10376/10376 [==============================] - 1s 49us/sample - loss: 0.0548 - accuracy: 0.9847 - val_loss: 0.3452 - val_accuracy: 0.9065\n",
      "Epoch 67/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0535 - accuracy: 0.9850 - val_loss: 0.3474 - val_accuracy: 0.9076\n",
      "Epoch 68/300\n",
      "10376/10376 [==============================] - 1s 56us/sample - loss: 0.0522 - accuracy: 0.9855 - val_loss: 0.3523 - val_accuracy: 0.9062\n",
      "Epoch 69/300\n",
      "10376/10376 [==============================] - 1s 49us/sample - loss: 0.0512 - accuracy: 0.9859 - val_loss: 0.3557 - val_accuracy: 0.9064\n",
      "Epoch 70/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0504 - accuracy: 0.9861 - val_loss: 0.3598 - val_accuracy: 0.9072\n",
      "Epoch 71/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0486 - accuracy: 0.9867 - val_loss: 0.3625 - val_accuracy: 0.9066\n",
      "Epoch 72/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0485 - accuracy: 0.9865 - val_loss: 0.3660 - val_accuracy: 0.9061\n",
      "Epoch 73/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0484 - accuracy: 0.9866 - val_loss: 0.3701 - val_accuracy: 0.9059\n",
      "Epoch 74/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0464 - accuracy: 0.9873 - val_loss: 0.3740 - val_accuracy: 0.9055\n",
      "Epoch 75/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0447 - accuracy: 0.9882 - val_loss: 0.3770 - val_accuracy: 0.9061\n",
      "Epoch 76/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0444 - accuracy: 0.9880 - val_loss: 0.3801 - val_accuracy: 0.9047\n",
      "Epoch 77/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0434 - accuracy: 0.9884 - val_loss: 0.3832 - val_accuracy: 0.9060\n",
      "Epoch 78/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0423 - accuracy: 0.9884 - val_loss: 0.3874 - val_accuracy: 0.9056\n",
      "Epoch 79/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0423 - accuracy: 0.9887 - val_loss: 0.3898 - val_accuracy: 0.9057\n",
      "Epoch 80/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0410 - accuracy: 0.9888 - val_loss: 0.3917 - val_accuracy: 0.9053\n",
      "Epoch 81/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0395 - accuracy: 0.9896 - val_loss: 0.3938 - val_accuracy: 0.9056\n",
      "Epoch 82/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0390 - accuracy: 0.9896 - val_loss: 0.4021 - val_accuracy: 0.9059\n",
      "Epoch 83/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0389 - accuracy: 0.9894 - val_loss: 0.4007 - val_accuracy: 0.9038\n",
      "Epoch 84/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0377 - accuracy: 0.9899 - val_loss: 0.4067 - val_accuracy: 0.9047\n",
      "Epoch 85/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0369 - accuracy: 0.9900 - val_loss: 0.4080 - val_accuracy: 0.9050\n",
      "Epoch 86/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0355 - accuracy: 0.9905 - val_loss: 0.4103 - val_accuracy: 0.9048\n",
      "Epoch 87/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.4146 - val_accuracy: 0.9045\n",
      "Epoch 88/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0345 - accuracy: 0.9907 - val_loss: 0.4188 - val_accuracy: 0.9053\n",
      "Epoch 89/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0346 - accuracy: 0.9908 - val_loss: 0.4212 - val_accuracy: 0.9041\n",
      "Epoch 90/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0338 - accuracy: 0.9908 - val_loss: 0.4228 - val_accuracy: 0.9045\n",
      "Epoch 91/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0331 - accuracy: 0.9911 - val_loss: 0.4251 - val_accuracy: 0.9047\n",
      "Epoch 92/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0324 - accuracy: 0.9911 - val_loss: 0.4280 - val_accuracy: 0.9039\n",
      "Epoch 93/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0319 - accuracy: 0.9912 - val_loss: 0.4316 - val_accuracy: 0.9031\n",
      "Epoch 94/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0316 - accuracy: 0.9912 - val_loss: 0.4346 - val_accuracy: 0.9028\n",
      "Epoch 95/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0310 - accuracy: 0.9915 - val_loss: 0.4390 - val_accuracy: 0.9034\n",
      "Epoch 96/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0298 - accuracy: 0.9920 - val_loss: 0.4443 - val_accuracy: 0.9038\n",
      "Epoch 97/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0292 - accuracy: 0.9921 - val_loss: 0.4436 - val_accuracy: 0.9035\n",
      "Epoch 98/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0288 - accuracy: 0.9922 - val_loss: 0.4463 - val_accuracy: 0.9034\n",
      "Epoch 99/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0290 - accuracy: 0.9920 - val_loss: 0.4520 - val_accuracy: 0.9029\n",
      "Epoch 100/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0282 - accuracy: 0.9923 - val_loss: 0.4514 - val_accuracy: 0.9028\n",
      "Epoch 101/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0271 - accuracy: 0.9926 - val_loss: 0.4527 - val_accuracy: 0.9037\n",
      "Epoch 102/300\n",
      "10376/10376 [==============================] - 1s 61us/sample - loss: 0.0266 - accuracy: 0.9929 - val_loss: 0.4576 - val_accuracy: 0.9033\n",
      "Epoch 103/300\n",
      "10376/10376 [==============================] - 1s 68us/sample - loss: 0.0256 - accuracy: 0.9932 - val_loss: 0.4598 - val_accuracy: 0.9039\n",
      "Epoch 104/300\n",
      "10376/10376 [==============================] - 1s 69us/sample - loss: 0.0258 - accuracy: 0.9929 - val_loss: 0.4643 - val_accuracy: 0.9009\n",
      "Epoch 105/300\n",
      "10376/10376 [==============================] - 1s 51us/sample - loss: 0.0253 - accuracy: 0.9932 - val_loss: 0.4672 - val_accuracy: 0.9046\n",
      "Epoch 106/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0250 - accuracy: 0.9935 - val_loss: 0.4724 - val_accuracy: 0.9033\n",
      "Epoch 107/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0238 - accuracy: 0.9935 - val_loss: 0.4715 - val_accuracy: 0.9026\n",
      "Epoch 108/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.4784 - val_accuracy: 0.9014\n",
      "Epoch 109/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0231 - accuracy: 0.9939 - val_loss: 0.4794 - val_accuracy: 0.9037\n",
      "Epoch 110/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0229 - accuracy: 0.9940 - val_loss: 0.4816 - val_accuracy: 0.9021\n",
      "Epoch 111/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.4870 - val_accuracy: 0.9012\n",
      "Epoch 112/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0218 - accuracy: 0.9942 - val_loss: 0.4902 - val_accuracy: 0.9006\n",
      "Epoch 113/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0216 - accuracy: 0.9942 - val_loss: 0.4926 - val_accuracy: 0.9019\n",
      "Epoch 114/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.4946 - val_accuracy: 0.9010\n",
      "Epoch 115/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0210 - accuracy: 0.9946 - val_loss: 0.4962 - val_accuracy: 0.9025\n",
      "Epoch 116/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0205 - accuracy: 0.9946 - val_loss: 0.4982 - val_accuracy: 0.9024\n",
      "Epoch 117/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0202 - accuracy: 0.9949 - val_loss: 0.5034 - val_accuracy: 0.9014\n",
      "Epoch 118/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0197 - accuracy: 0.9947 - val_loss: 0.5031 - val_accuracy: 0.9026\n",
      "Epoch 119/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.5098 - val_accuracy: 0.9010\n",
      "Epoch 120/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0190 - accuracy: 0.9950 - val_loss: 0.5070 - val_accuracy: 0.9012\n",
      "Epoch 121/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0187 - accuracy: 0.9952 - val_loss: 0.5130 - val_accuracy: 0.9002\n",
      "Epoch 122/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0198 - accuracy: 0.9946 - val_loss: 0.5169 - val_accuracy: 0.9007\n",
      "Epoch 123/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0186 - accuracy: 0.9951 - val_loss: 0.5179 - val_accuracy: 0.9004\n",
      "Epoch 124/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0176 - accuracy: 0.9955 - val_loss: 0.5215 - val_accuracy: 0.9006\n",
      "Epoch 125/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.9955 - val_loss: 0.5221 - val_accuracy: 0.9007\n",
      "Epoch 126/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.9958 - val_loss: 0.5242 - val_accuracy: 0.9016\n",
      "Epoch 127/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.9953 - val_loss: 0.5277 - val_accuracy: 0.8997\n",
      "Epoch 128/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.9958 - val_loss: 0.5297 - val_accuracy: 0.9024\n",
      "Epoch 129/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.5309 - val_accuracy: 0.9015\n",
      "Epoch 130/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0157 - accuracy: 0.9960 - val_loss: 0.5355 - val_accuracy: 0.9000\n",
      "Epoch 131/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0153 - accuracy: 0.9962 - val_loss: 0.5380 - val_accuracy: 0.8994\n",
      "Epoch 132/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0156 - accuracy: 0.9960 - val_loss: 0.5446 - val_accuracy: 0.9008\n",
      "Epoch 133/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0152 - accuracy: 0.9961 - val_loss: 0.5425 - val_accuracy: 0.9013\n",
      "Epoch 134/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0152 - accuracy: 0.9959 - val_loss: 0.5435 - val_accuracy: 0.9006\n",
      "Epoch 135/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.5470 - val_accuracy: 0.9007\n",
      "Epoch 136/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0146 - accuracy: 0.9962 - val_loss: 0.5489 - val_accuracy: 0.9005\n",
      "Epoch 137/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0148 - accuracy: 0.9961 - val_loss: 0.5514 - val_accuracy: 0.9001\n",
      "Epoch 138/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0142 - accuracy: 0.9963 - val_loss: 0.5585 - val_accuracy: 0.8999\n",
      "Epoch 139/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0135 - accuracy: 0.9965 - val_loss: 0.5558 - val_accuracy: 0.8998\n",
      "Epoch 140/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0134 - accuracy: 0.9966 - val_loss: 0.5589 - val_accuracy: 0.9006\n",
      "Epoch 141/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.5636 - val_accuracy: 0.9012\n",
      "Epoch 142/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.5656 - val_accuracy: 0.9003\n",
      "Epoch 143/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0129 - accuracy: 0.9967 - val_loss: 0.5693 - val_accuracy: 0.8988\n",
      "Epoch 144/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.5710 - val_accuracy: 0.9007\n",
      "Epoch 145/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0125 - accuracy: 0.9968 - val_loss: 0.5736 - val_accuracy: 0.9003\n",
      "Epoch 146/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.5727 - val_accuracy: 0.9000\n",
      "Epoch 147/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0117 - accuracy: 0.9971 - val_loss: 0.5799 - val_accuracy: 0.8995\n",
      "Epoch 148/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.5767 - val_accuracy: 0.9003\n",
      "Epoch 149/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.5845 - val_accuracy: 0.8995\n",
      "Epoch 150/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0115 - accuracy: 0.9969 - val_loss: 0.5862 - val_accuracy: 0.9011\n",
      "Epoch 151/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0117 - accuracy: 0.9968 - val_loss: 0.5834 - val_accuracy: 0.9003\n",
      "Epoch 152/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.5878 - val_accuracy: 0.8994\n",
      "Epoch 153/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.5908 - val_accuracy: 0.8999\n",
      "Epoch 154/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.5952 - val_accuracy: 0.8995\n",
      "Epoch 155/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0114 - accuracy: 0.9971 - val_loss: 0.5957 - val_accuracy: 0.8995\n",
      "Epoch 156/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.5970 - val_accuracy: 0.8994\n",
      "Epoch 157/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.6040 - val_accuracy: 0.8999\n",
      "Epoch 158/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.6037 - val_accuracy: 0.9004\n",
      "Epoch 159/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0099 - accuracy: 0.9975 - val_loss: 0.6049 - val_accuracy: 0.8992\n",
      "Epoch 160/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.6060 - val_accuracy: 0.9011\n",
      "Epoch 161/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.6137 - val_accuracy: 0.8981\n",
      "Epoch 162/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.6098 - val_accuracy: 0.9007\n",
      "Epoch 163/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.6119 - val_accuracy: 0.9007\n",
      "Epoch 164/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.6146 - val_accuracy: 0.8996\n",
      "Epoch 165/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.6134 - val_accuracy: 0.9000\n",
      "Epoch 166/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.6125 - val_accuracy: 0.8996\n",
      "Epoch 167/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.6202 - val_accuracy: 0.8986\n",
      "Epoch 168/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.6234 - val_accuracy: 0.8996\n",
      "Epoch 169/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.6261 - val_accuracy: 0.8988\n",
      "Epoch 170/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.6269 - val_accuracy: 0.9009\n",
      "Epoch 171/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.6320 - val_accuracy: 0.8998\n",
      "Epoch 172/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.6332 - val_accuracy: 0.8999\n",
      "Epoch 173/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.6344 - val_accuracy: 0.8993\n",
      "Epoch 174/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.6447 - val_accuracy: 0.8994\n",
      "Epoch 175/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.6412 - val_accuracy: 0.8982\n",
      "Epoch 176/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.6432 - val_accuracy: 0.8991\n",
      "Epoch 177/300\n",
      "10376/10376 [==============================] - 1s 52us/sample - loss: 0.0085 - accuracy: 0.9978 - val_loss: 0.6414 - val_accuracy: 0.8996\n",
      "Epoch 178/300\n",
      "10376/10376 [==============================] - 1s 54us/sample - loss: 0.0077 - accuracy: 0.9981 - val_loss: 0.6429 - val_accuracy: 0.8992\n",
      "Epoch 179/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0081 - accuracy: 0.9978 - val_loss: 0.6457 - val_accuracy: 0.8982\n",
      "Epoch 180/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0081 - accuracy: 0.9979 - val_loss: 0.6483 - val_accuracy: 0.8984\n",
      "Epoch 181/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.6503 - val_accuracy: 0.8993\n",
      "Epoch 182/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0074 - accuracy: 0.9981 - val_loss: 0.6522 - val_accuracy: 0.8992\n",
      "Epoch 183/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0079 - accuracy: 0.9979 - val_loss: 0.6542 - val_accuracy: 0.8989\n",
      "Epoch 184/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.6639 - val_accuracy: 0.8985\n",
      "Epoch 185/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0078 - accuracy: 0.9980 - val_loss: 0.6534 - val_accuracy: 0.8994\n",
      "Epoch 186/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0075 - accuracy: 0.9981 - val_loss: 0.6557 - val_accuracy: 0.9004\n",
      "Epoch 187/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.6627 - val_accuracy: 0.9010\n",
      "Epoch 188/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.6648 - val_accuracy: 0.9001\n",
      "Epoch 189/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.6653 - val_accuracy: 0.8985\n",
      "Epoch 190/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.6727 - val_accuracy: 0.8977\n",
      "Epoch 191/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.6671 - val_accuracy: 0.8985\n",
      "Epoch 192/300\n",
      "10376/10376 [==============================] - 1s 58us/sample - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.6663 - val_accuracy: 0.8985\n",
      "Epoch 193/300\n",
      "10376/10376 [==============================] - 1s 56us/sample - loss: 0.0068 - accuracy: 0.9983 - val_loss: 0.6691 - val_accuracy: 0.8989\n",
      "Epoch 194/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.6680 - val_accuracy: 0.8986\n",
      "Epoch 195/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.6722 - val_accuracy: 0.9000\n",
      "Epoch 196/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.6781 - val_accuracy: 0.8987\n",
      "Epoch 197/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.6804 - val_accuracy: 0.8995\n",
      "Epoch 198/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.6827 - val_accuracy: 0.9017\n",
      "Epoch 199/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.6835 - val_accuracy: 0.8999\n",
      "Epoch 200/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.6906 - val_accuracy: 0.8988\n",
      "Epoch 201/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.6928 - val_accuracy: 0.8992\n",
      "Epoch 202/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.6953 - val_accuracy: 0.8986\n",
      "Epoch 203/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.6911 - val_accuracy: 0.8981\n",
      "Epoch 204/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.6948 - val_accuracy: 0.8972\n",
      "Epoch 205/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.6952 - val_accuracy: 0.8983\n",
      "Epoch 206/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.6980 - val_accuracy: 0.8989\n",
      "Epoch 207/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.7005 - val_accuracy: 0.8984\n",
      "Epoch 208/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.7019 - val_accuracy: 0.8995\n",
      "Epoch 209/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.7031 - val_accuracy: 0.8976\n",
      "Epoch 210/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.7041 - val_accuracy: 0.8998\n",
      "Epoch 211/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.7072 - val_accuracy: 0.9000\n",
      "Epoch 212/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.7002 - val_accuracy: 0.8999\n",
      "Epoch 213/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.7042 - val_accuracy: 0.8994\n",
      "Epoch 214/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.7098 - val_accuracy: 0.8994\n",
      "Epoch 215/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.7131 - val_accuracy: 0.8980\n",
      "Epoch 216/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.7053 - val_accuracy: 0.8997\n",
      "Epoch 217/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.7125 - val_accuracy: 0.8986\n",
      "Epoch 218/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.7153 - val_accuracy: 0.8986\n",
      "Epoch 219/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.7138 - val_accuracy: 0.8971\n",
      "Epoch 220/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.7164 - val_accuracy: 0.8989\n",
      "Epoch 221/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.7188 - val_accuracy: 0.8992\n",
      "Epoch 222/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.7199 - val_accuracy: 0.8968\n",
      "Epoch 223/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.7211 - val_accuracy: 0.8993\n",
      "Epoch 224/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.7216 - val_accuracy: 0.9000\n",
      "Epoch 225/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.7249 - val_accuracy: 0.8999\n",
      "Epoch 226/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.7328 - val_accuracy: 0.8997\n",
      "Epoch 227/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.7309 - val_accuracy: 0.8988\n",
      "Epoch 228/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.7333 - val_accuracy: 0.8992\n",
      "Epoch 229/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.7382 - val_accuracy: 0.8992\n",
      "Epoch 230/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.7372 - val_accuracy: 0.8996\n",
      "Epoch 231/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.7303 - val_accuracy: 0.8991\n",
      "Epoch 232/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.7380 - val_accuracy: 0.9003\n",
      "Epoch 233/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.7409 - val_accuracy: 0.8986\n",
      "Epoch 234/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.7425 - val_accuracy: 0.8995\n",
      "Epoch 235/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.7462 - val_accuracy: 0.8993\n",
      "Epoch 236/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.7506 - val_accuracy: 0.8984\n",
      "Epoch 237/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.7515 - val_accuracy: 0.8991\n",
      "Epoch 238/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.7491 - val_accuracy: 0.8974\n",
      "Epoch 239/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.7510 - val_accuracy: 0.8989\n",
      "Epoch 240/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.7572 - val_accuracy: 0.8946\n",
      "Epoch 241/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.7594 - val_accuracy: 0.8976\n",
      "Epoch 242/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.7522 - val_accuracy: 0.8976\n",
      "Epoch 243/300\n",
      "10376/10376 [==============================] - 1s 53us/sample - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.7590 - val_accuracy: 0.8970\n",
      "Epoch 244/300\n",
      "10376/10376 [==============================] - 1s 55us/sample - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.7519 - val_accuracy: 0.8987\n",
      "Epoch 245/300\n",
      "10376/10376 [==============================] - 0s 47us/sample - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.7539 - val_accuracy: 0.9009\n",
      "Epoch 246/300\n",
      "10376/10376 [==============================] - 1s 52us/sample - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.7519 - val_accuracy: 0.8971\n",
      "Epoch 247/300\n",
      "10376/10376 [==============================] - 1s 54us/sample - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.7717 - val_accuracy: 0.8969\n",
      "Epoch 248/300\n",
      "10376/10376 [==============================] - 1s 53us/sample - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.7648 - val_accuracy: 0.8983\n",
      "Epoch 249/300\n",
      "10376/10376 [==============================] - 1s 78us/sample - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.7604 - val_accuracy: 0.9005\n",
      "Epoch 250/300\n",
      "10376/10376 [==============================] - 1s 60us/sample - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.7605 - val_accuracy: 0.8991\n",
      "Epoch 251/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.7668 - val_accuracy: 0.8990\n",
      "Epoch 252/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.7600 - val_accuracy: 0.9002\n",
      "Epoch 253/300\n",
      "10376/10376 [==============================] - 0s 44us/sample - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.7730 - val_accuracy: 0.8988\n",
      "Epoch 254/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.7726 - val_accuracy: 0.8985\n",
      "Epoch 255/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.7793 - val_accuracy: 0.8999\n",
      "Epoch 256/300\n",
      "10376/10376 [==============================] - 1s 58us/sample - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.7701 - val_accuracy: 0.9004\n",
      "Epoch 257/300\n",
      "10376/10376 [==============================] - 1s 50us/sample - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.7760 - val_accuracy: 0.8985\n",
      "Epoch 258/300\n",
      "10376/10376 [==============================] - 0s 46us/sample - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.7743 - val_accuracy: 0.8989\n",
      "Epoch 259/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.7787 - val_accuracy: 0.8991\n",
      "Epoch 260/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.7755 - val_accuracy: 0.9003\n",
      "Epoch 261/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.7779 - val_accuracy: 0.8988\n",
      "Epoch 262/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.7817 - val_accuracy: 0.9005\n",
      "Epoch 263/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.7839 - val_accuracy: 0.8982\n",
      "Epoch 264/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.7805 - val_accuracy: 0.8968\n",
      "Epoch 265/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.7844 - val_accuracy: 0.9001\n",
      "Epoch 266/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.7773 - val_accuracy: 0.8997\n",
      "Epoch 267/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.7789 - val_accuracy: 0.8995\n",
      "Epoch 268/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.7832 - val_accuracy: 0.8981\n",
      "Epoch 269/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.7833 - val_accuracy: 0.8990\n",
      "Epoch 270/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.7823 - val_accuracy: 0.8978\n",
      "Epoch 271/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.7846 - val_accuracy: 0.8992\n",
      "Epoch 272/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.7852 - val_accuracy: 0.8997\n",
      "Epoch 273/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.7910 - val_accuracy: 0.8989\n",
      "Epoch 274/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.7912 - val_accuracy: 0.8978\n",
      "Epoch 275/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.7927 - val_accuracy: 0.8981\n",
      "Epoch 276/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.7911 - val_accuracy: 0.8987\n",
      "Epoch 277/300\n",
      "10376/10376 [==============================] - 0s 45us/sample - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.7928 - val_accuracy: 0.9000\n",
      "Epoch 278/300\n",
      "10376/10376 [==============================] - 1s 59us/sample - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.7895 - val_accuracy: 0.8998\n",
      "Epoch 279/300\n",
      "10376/10376 [==============================] - 1s 51us/sample - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.7925 - val_accuracy: 0.8994\n",
      "Epoch 280/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.7990 - val_accuracy: 0.8985\n",
      "Epoch 281/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.7933 - val_accuracy: 0.9002\n",
      "Epoch 282/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.7965 - val_accuracy: 0.8996\n",
      "Epoch 283/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.8003 - val_accuracy: 0.8981\n",
      "Epoch 284/300\n",
      "10376/10376 [==============================] - 0s 38us/sample - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.7950 - val_accuracy: 0.9001\n",
      "Epoch 285/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.7988 - val_accuracy: 0.8992\n",
      "Epoch 286/300\n",
      "10376/10376 [==============================] - 0s 39us/sample - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.8039 - val_accuracy: 0.8988\n",
      "Epoch 287/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.8058 - val_accuracy: 0.9003\n",
      "Epoch 288/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.8129 - val_accuracy: 0.8987\n",
      "Epoch 289/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.8073 - val_accuracy: 0.9001\n",
      "Epoch 290/300\n",
      "10376/10376 [==============================] - 0s 43us/sample - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.8081 - val_accuracy: 0.8995\n",
      "Epoch 291/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.8080 - val_accuracy: 0.9008\n",
      "Epoch 292/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.8113 - val_accuracy: 0.8982\n",
      "Epoch 293/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.8203 - val_accuracy: 0.8989\n",
      "Epoch 294/300\n",
      "10376/10376 [==============================] - 0s 42us/sample - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.8095 - val_accuracy: 0.8997\n",
      "Epoch 295/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.8122 - val_accuracy: 0.9002\n",
      "Epoch 296/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.8174 - val_accuracy: 0.9010\n",
      "Epoch 297/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.8063 - val_accuracy: 0.9004\n",
      "Epoch 298/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.8162 - val_accuracy: 0.8986\n",
      "Epoch 299/300\n",
      "10376/10376 [==============================] - 0s 41us/sample - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.8116 - val_accuracy: 0.8998\n",
      "Epoch 300/300\n",
      "10376/10376 [==============================] - 0s 40us/sample - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.8184 - val_accuracy: 0.9022\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_array,\n",
    "                    y_train_array,\n",
    "                    epochs=300,\n",
    "                    batch_size=258,\n",
    "                    validation_data=(X_valid_array, y_valid_array),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 0s 16us/sample\n",
      "Results on y prediction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.391288</td>\n",
       "      <td>0.097758</td>\n",
       "      <td>0.578962</td>\n",
       "      <td>0.582642</td>\n",
       "      <td>0.580796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.395914</td>\n",
       "      <td>0.096826</td>\n",
       "      <td>0.584641</td>\n",
       "      <td>0.576562</td>\n",
       "      <td>0.580573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.398227</td>\n",
       "      <td>0.096055</td>\n",
       "      <td>0.590386</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.578398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.399769</td>\n",
       "      <td>0.094834</td>\n",
       "      <td>0.598057</td>\n",
       "      <td>0.561360</td>\n",
       "      <td>0.579127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.405551</td>\n",
       "      <td>0.093132</td>\n",
       "      <td>0.610310</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>0.578450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold  Accuracy  Hamming loss  Precision    Recall  F1-measure\n",
       "0        0.5  0.391288      0.097758   0.578962  0.582642    0.580796\n",
       "1        0.6  0.395914      0.096826   0.584641  0.576562    0.580573\n",
       "2        0.7  0.398227      0.096055   0.590386  0.566888    0.578398\n",
       "3        0.8  0.399769      0.094834   0.598057  0.561360    0.579127\n",
       "4        0.9  0.405551      0.093132   0.610310  0.549751    0.578450"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_valid_array, batch_size=batch_size, verbose=1)\n",
    "\n",
    "predictions = pred\n",
    "\n",
    "predictions_results = []\n",
    "thresholds=[0.5,0.6,0.7,0.8,0.9] #np.arange(.5, 1, 0.1).tolist()\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "    accuracy = accuracy_score(y_valid_array, pred, normalize=True, sample_weight=None)#average='micro')\n",
    "    hamming = hamming_loss(y_valid_array, pred)\n",
    "    precision = precision_score(y_valid_array, pred, average='micro')\n",
    "    recall = recall_score(y_valid_array, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_array, pred, average='micro')\n",
    "    case= {'Threshold': val,\n",
    "           'Accuracy': accuracy,\n",
    "           'Hamming loss': hamming,\n",
    "           'Precision': precision,\n",
    "           'Recall': recall,\n",
    "           'F1-measure': f1}\n",
    "    predictions_results.append(case)\n",
    "print(\"Results on y prediction:\")\n",
    "pd.DataFrame(predictions_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[512], input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "keras_layer_5 (KerasLayer)   (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 512, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 512, 128)          384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 103, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 103, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 21, 128)           65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 256,931,212\n",
      "Trainable params: 256,931,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = tf.keras.layers.Input(shape=(), name=\"Input\", dtype=tf.string)\n",
    "x = hub_layer(input)\n",
    "x = tf.keras.layers.Reshape(input_shape=(512,), target_shape=(512, 1))(x)\n",
    "x = tf.keras.layers.Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(5, padding='same')(x)\n",
    "x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(5, padding='same')(x)\n",
    "x = tf.keras.layers.Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(40, padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(12, activation='sigmoid')(x)\n",
    "m = tf.keras.Model(input, output)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10376 samples, validate on 2594 samples\n",
      "Epoch 1/5\n",
      "10376/10376 [==============================] - 81s 8ms/sample - loss: 0.4552 - accuracy: 0.8634 - val_loss: 0.3497 - val_accuracy: 0.8838\n",
      "Epoch 2/5\n",
      "10376/10376 [==============================] - 80s 8ms/sample - loss: 0.3567 - accuracy: 0.8830 - val_loss: 0.3466 - val_accuracy: 0.8838\n",
      "Epoch 3/5\n",
      "10376/10376 [==============================] - 75s 7ms/sample - loss: 0.3522 - accuracy: 0.8830 - val_loss: 0.3447 - val_accuracy: 0.8838\n",
      "Epoch 4/5\n",
      "10376/10376 [==============================] - 76s 7ms/sample - loss: 0.3488 - accuracy: 0.8830 - val_loss: 0.3424 - val_accuracy: 0.8838\n",
      "Epoch 5/5\n",
      "10376/10376 [==============================] - 77s 7ms/sample - loss: 0.3434 - accuracy: 0.8830 - val_loss: 0.3388 - val_accuracy: 0.8838\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(X_train_array,\n",
    "                    y_train_array,\n",
    "                    epochs=5,\n",
    "                    batch_size=258,\n",
    "                    validation_data=(X_valid_array, y_valid_array),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 2s 599us/sample\n",
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.1662, Recall: 0.8295, F1-measure: 0.2769\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2123, Recall: 0.3007, F1-measure: 0.2489\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.2339, Recall: 0.1669, F1-measure: 0.1948\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = m.predict(X_valid_array, batch_size=512, verbose=1)\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid_array, pred, average='micro')\n",
    "    recall = recall_score(y_valid_array, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_array, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
