{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model - CNN Multi-Label Text Classification\n",
    "### Universal Sentence Encoder with Sub Themes (Right Now Ran for Main Themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling1D, MaxPool1D, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Q1 = pd.read_excel('../data/interim/X_train_Q1_clean.xlsx')\n",
    "X_valid_Q1 = pd.read_excel('../data/interim/X_valid_Q1_clean.xlsx')\n",
    "\n",
    "y_train_Q1 = pd.read_excel('../data/interim/y_train_Q1.xlsx')\n",
    "y_valid_Q1 = pd.read_excel('../data/interim/y_valid_Q1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Unified Dataframe for CNN Ready Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([X_train_Q1, y_train_Q1.iloc[:,0:12]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>CPD</th>\n",
       "      <th>CB</th>\n",
       "      <th>EWC</th>\n",
       "      <th>Exec</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SP</th>\n",
       "      <th>RE</th>\n",
       "      <th>Sup</th>\n",
       "      <th>SW</th>\n",
       "      <th>TEPE</th>\n",
       "      <th>VMG</th>\n",
       "      <th>OTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to be real about diversity, you need to create...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep the building warmer and provide warm wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better communication from the top down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be beneficial if Management did not m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more education applicable to my job</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  CPD  CB  EWC  Exec  FEW  \\\n",
       "0  to be real about diversity, you need to create...    0   0    1     0    0   \n",
       "1  Keep the building warmer and provide warm wate...    0   0    0     0    0   \n",
       "2             better communication from the top down    0   0    0     1    0   \n",
       "3  It would be beneficial if Management did not m...    0   0    0     0    0   \n",
       "4                more education applicable to my job    1   0    0     0    0   \n",
       "\n",
       "   SP  RE  Sup  SW  TEPE  VMG  OTH  \n",
       "0   0   0    0   0     0    0    0  \n",
       "1   0   0    0   0     1    0    0  \n",
       "2   0   0    0   0     0    0    0  \n",
       "3   0   1    0   0     0    0    0  \n",
       "4   0   0    0   0     0    0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in data_df['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "data_df['preprocessed_comments']=preprocessed_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>CPD</th>\n",
       "      <th>CB</th>\n",
       "      <th>EWC</th>\n",
       "      <th>Exec</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SP</th>\n",
       "      <th>RE</th>\n",
       "      <th>Sup</th>\n",
       "      <th>SW</th>\n",
       "      <th>TEPE</th>\n",
       "      <th>VMG</th>\n",
       "      <th>OTH</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to be real about diversity, you need to create...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>real diversity you need create seats table mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep the building warmer and provide warm wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>keep building warmer provide warm water bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better communication from the top down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>better communication top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be beneficial if Management did not m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>would beneficial management not micro manage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more education applicable to my job</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>education applicable job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  CPD  CB  EWC  Exec  FEW  \\\n",
       "0  to be real about diversity, you need to create...    0   0    1     0    0   \n",
       "1  Keep the building warmer and provide warm wate...    0   0    0     0    0   \n",
       "2             better communication from the top down    0   0    0     1    0   \n",
       "3  It would be beneficial if Management did not m...    0   0    0     0    0   \n",
       "4                more education applicable to my job    1   0    0     0    0   \n",
       "\n",
       "   SP  RE  Sup  SW  TEPE  VMG  OTH  \\\n",
       "0   0   0    0   0     0    0    0   \n",
       "1   0   0    0   0     1    0    0   \n",
       "2   0   0    0   0     0    0    0   \n",
       "3   0   1    0   0     0    0    0   \n",
       "4   0   0    0   0     0    0    0   \n",
       "\n",
       "                               preprocessed_comments  \n",
       "0  real diversity you need create seats table mea...  \n",
       "1   keep building warmer provide warm water bathroom  \n",
       "2                           better communication top  \n",
       "3       would beneficial management not micro manage  \n",
       "4                           education applicable job  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Pre-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_comments(text, \n",
    "                        min_token_len = 2, \n",
    "                        irrelevant_pos = ['PRON', 'SPACE', 'PUNCT', 'ADV', \n",
    "                                          'ADP', 'CCONJ', 'AUX', 'PRP'],\n",
    "                        avoid_entities = ['PERSON', 'ORG', 'LOC', 'GPE']):\n",
    "# note: Didn't use the following options in the `preprocess_comments`...\n",
    "#    - 'PROPN' because it erases proper names as 'George', but also words as orange.\n",
    "#    - 'DET' since it removes the word 'no', which changes the meaning of a sentence.\n",
    "# *for more information see link: https://universaldependencies.org/u/pos/\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, irrelevant_pos and avoid_entities, carries out \n",
    "    preprocessing of the text and returns list of preprocessed text. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (list) \n",
    "        the list of text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    avoid_entities : (list)\n",
    "        a list of entity labels to be avoided\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (list) list of preprocessed text\n",
    "    \n",
    "    Example\n",
    "    -------------\n",
    "    >>> example = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]\n",
    "\n",
    "    >>> preprocess(example)\n",
    "    (output:) ['hello love swimming', 'good cook', 'contact']\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    others = [\"'s\", \"the\", \"that\", \"this\", \"to\", \"-PRON-\"]\n",
    "    # I add \"-PRON-\" that erase \"my\", \"your\", etc. other way to erase them is to\n",
    "    #   use adding 'DET' to irrelevant_pos but it would erase the word 'no' too.\n",
    "    \n",
    "    for sent in text:\n",
    "        sent = sent.lower()\n",
    "        sent = re.sub(r\"facebook\", \"social media\", sent)\n",
    "        sent = re.sub(r\"twitter\", \"social media\", sent)\n",
    "        sent = re.sub(r\"instagram\", \"social media\", sent)\n",
    "        sent = re.sub(r\"whatsapp\", \"social media\", sent)\n",
    "        sent = re.sub(r\"linkedin\", \"social media\", sent)\n",
    "        sent = re.sub(r\"snapchat\", \"social media\", sent)\n",
    "        \n",
    "        result_sent = []\n",
    "        doc = nlp(sent)\n",
    "        entities = [str(ent) for ent in doc.ents if ent.label_ in avoid_entities]\n",
    "        # This helps to detect names of persons, organization and dates\n",
    "        \n",
    "        for token in doc:            \n",
    "            if (token.like_email or\n",
    "                token.like_url or\n",
    "                token.pos_ in irrelevant_pos or\n",
    "                str(token) in entities or\n",
    "                str(token.lemma_) in others or\n",
    "                len(token) < min_token_len):\n",
    "                continue\n",
    "            else:\n",
    "                result_sent.append(token.lemma_)\n",
    "        result.append(\" \".join(result_sent))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['preprocessed_comments']=preprocess_comments(data_df['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>CPD</th>\n",
       "      <th>CB</th>\n",
       "      <th>EWC</th>\n",
       "      <th>Exec</th>\n",
       "      <th>FEW</th>\n",
       "      <th>SP</th>\n",
       "      <th>RE</th>\n",
       "      <th>Sup</th>\n",
       "      <th>SW</th>\n",
       "      <th>TEPE</th>\n",
       "      <th>VMG</th>\n",
       "      <th>OTH</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to be real about diversity, you need to create...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>real diversity you need create seats table mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep the building warmer and provide warm wate...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>keep building warmer provide warm water bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better communication from the top down</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>better communication top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It would be beneficial if Management did not m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>would beneficial management not micro manage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>more education applicable to my job</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>education applicable job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Comment  CPD  CB  EWC  Exec  FEW  \\\n",
       "0  to be real about diversity, you need to create...    0   0    1     0    0   \n",
       "1  Keep the building warmer and provide warm wate...    0   0    0     0    0   \n",
       "2             better communication from the top down    0   0    0     1    0   \n",
       "3  It would be beneficial if Management did not m...    0   0    0     0    0   \n",
       "4                more education applicable to my job    1   0    0     0    0   \n",
       "\n",
       "   SP  RE  Sup  SW  TEPE  VMG  OTH  \\\n",
       "0   0   0    0   0     0    0    0   \n",
       "1   0   0    0   0     1    0    0   \n",
       "2   0   0    0   0     0    0    0   \n",
       "3   0   1    0   0     0    0    0   \n",
       "4   0   0    0   0     0    0    0   \n",
       "\n",
       "                               preprocessed_comments  \n",
       "0  real diversity you need create seats table mea...  \n",
       "1   keep building warmer provide warm water bathroom  \n",
       "2                           better communication top  \n",
       "3       would beneficial management not micro manage  \n",
       "4                           education applicable job  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[['preprocessed_comments']]\n",
    "y_train = data_df.drop(['Comment', 'preprocessed_comments'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max length of sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_len(x):\n",
    "    a=x.split()\n",
    "    return len(a)\n",
    "\n",
    "max_len = max(data_df['Comment'].apply(max_len))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11919\n"
     ]
    }
   ],
   "source": [
    "vect=Tokenizer()\n",
    "vect.fit_on_texts(X_train['preprocessed_comments'])\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universal Sentence Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(X_train['preprocessed_comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03669859, -0.07417478,  0.03231422, ...,  0.01621998,\n",
       "         0.00728293,  0.00265464],\n",
       "       [ 0.05218186, -0.01258108, -0.04657765, ...,  0.05539099,\n",
       "         0.05895472, -0.0171202 ],\n",
       "       [-0.01777638, -0.04568463,  0.00473103, ..., -0.00073464,\n",
       "        -0.08261821,  0.05859691],\n",
       "       ...,\n",
       "       [ 0.03823395,  0.01441879,  0.06019125, ...,  0.06164353,\n",
       "        -0.02452262, -0.01176131],\n",
       "       [-0.03514517, -0.04318713, -0.02938136, ...,  0.0648592 ,\n",
       "        -0.05883494,  0.01129983],\n",
       "       [ 0.02614759, -0.05074428,  0.00971   , ...,  0.01864268,\n",
       "         0.05439513,  0.03895477]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 512)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding to make all sequences of same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  504   585    36 ...     0     0     0]\n",
      " [  134    54  3393 ...     0     0     0]\n",
      " [    7    29   234 ...     0     0     0]\n",
      " ...\n",
      " [  697     4    12 ...     0     0     0]\n",
      " [  476  2745 11917 ...     0     0     0]\n",
      " [ 1147   593   791 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs_train = vect.texts_to_sequences(X_train['preprocessed_comments'])\n",
    "max_length = vocab_size\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_len, padding='post')\n",
    "print(padded_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 150)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "\n",
    "max_features = embedding_matrix.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                        trainable=False, input_length=maxlen))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid',activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(hidden_dims, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_docs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model.fit(padded_docs_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Testing begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 512, 512)          6102528   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                6156      \n",
      "=================================================================\n",
      "Total params: 6,108,684\n",
      "Trainable params: 6,108,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_size, input_length=embed_size))\n",
    "#model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(hidden_dims, activation = 'relu'))\n",
    "#model.add(Dropout(0.6))\n",
    "model.add(Dense(n_class, activation = 'sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8819 samples, validate on 1557 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedding_matrix, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Simple Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 150\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 10\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 10376)             5322888   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 12)                124524    \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 5,447,412\n",
      "Trainable params: 5,447,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(max_features, input_shape=(512,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(n_class))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8819 samples, validate on 1557 samples\n",
      "Epoch 1/10\n",
      "8819/8819 [==============================] - 3s 352us/step - loss: 0.2990 - acc: 0.8945 - val_loss: 0.2108 - val_acc: 0.9231\n",
      "Epoch 2/10\n",
      "8819/8819 [==============================] - 2s 181us/step - loss: 0.1971 - acc: 0.9254 - val_loss: 0.1965 - val_acc: 0.9274\n",
      "Epoch 3/10\n",
      "8819/8819 [==============================] - 2s 175us/step - loss: 0.1824 - acc: 0.9307 - val_loss: 0.1920 - val_acc: 0.9291\n",
      "Epoch 4/10\n",
      "8819/8819 [==============================] - 2s 176us/step - loss: 0.1730 - acc: 0.9337 - val_loss: 0.1896 - val_acc: 0.9281\n",
      "Epoch 5/10\n",
      "8819/8819 [==============================] - 2s 176us/step - loss: 0.1650 - acc: 0.9362 - val_loss: 0.1880 - val_acc: 0.9291\n",
      "Epoch 6/10\n",
      "8819/8819 [==============================] - 2s 177us/step - loss: 0.1565 - acc: 0.9395 - val_loss: 0.1894 - val_acc: 0.9281\n",
      "Epoch 7/10\n",
      "8819/8819 [==============================] - 2s 176us/step - loss: 0.1473 - acc: 0.9425 - val_loss: 0.1899 - val_acc: 0.9268\n",
      "Epoch 8/10\n",
      "8819/8819 [==============================] - 2s 175us/step - loss: 0.1381 - acc: 0.9466 - val_loss: 0.1901 - val_acc: 0.9291\n",
      "Epoch 9/10\n",
      "8819/8819 [==============================] - 2s 177us/step - loss: 0.1279 - acc: 0.9506 - val_loss: 0.1896 - val_acc: 0.9289\n",
      "Epoch 10/10\n",
      "8819/8819 [==============================] - 2s 176us/step - loss: 0.1172 - acc: 0.9553 - val_loss: 0.1915 - val_acc: 0.9287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b07d6a910>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedding_matrix, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18472839403630406, 0.9315407588193301]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.concat([X_valid_Q1, y_valid_Q1.iloc[:,:12]], axis = 1)\n",
    "\n",
    "# pre-processing test data\n",
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df_valid['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "    \n",
    "    \n",
    "df_valid['preprocessed_comments']=preprocessed_synopsis\n",
    "\n",
    "# creating X and Y\n",
    "X_valid = df_valid[['preprocessed_comments']]\n",
    "y_valid = df_valid.drop(columns=['Comment', 'preprocessed_comments'])\n",
    "\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# creating embedding for validation\n",
    "embeddings_valid = embed(X_valid['preprocessed_comments'])\n",
    "embedding_matrix_valid = np.array(embeddings_valid)\n",
    "\n",
    "score = model.evaluate(embedding_matrix_valid,y_valid)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 1s 199us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(embedding_matrix_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For threshold:  0.1\n",
      "Micro-average quality numbers\n",
      "Precision: 0.4625, Recall: 0.8405, F1-measure: 0.5967\n",
      "\n",
      "\n",
      "For threshold:  0.2\n",
      "Micro-average quality numbers\n",
      "Precision: 0.5811, Recall: 0.7532, F1-measure: 0.6561\n",
      "\n",
      "\n",
      "For threshold:  0.3\n",
      "Micro-average quality numbers\n",
      "Precision: 0.6652, Recall: 0.6882, F1-measure: 0.6765\n",
      "\n",
      "\n",
      "For threshold:  0.4\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7252, Recall: 0.6332, F1-measure: 0.6761\n",
      "\n",
      "\n",
      "For threshold:  0.5\n",
      "Micro-average quality numbers\n",
      "Precision: 0.7749, Recall: 0.5793, F1-measure: 0.6630\n",
      "\n",
      "\n",
      "For threshold:  0.6\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8161, Recall: 0.5213, F1-measure: 0.6362\n",
      "\n",
      "\n",
      "For threshold:  0.7\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8603, Recall: 0.4666, F1-measure: 0.6050\n",
      "\n",
      "\n",
      "For threshold:  0.8\n",
      "Micro-average quality numbers\n",
      "Precision: 0.8910, Recall: 0.3930, F1-measure: 0.5455\n",
      "\n",
      "\n",
      "For threshold:  0.9\n",
      "Micro-average quality numbers\n",
      "Precision: 0.9386, Recall: 0.3085, F1-measure: 0.4643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "#predictions=model.predict([padded_docs_test])\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid, pred, average='micro')\n",
    "    recall = recall_score(y_valid, pred, average='micro')\n",
    "    f1 = f1_score(y_valid, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[512])\n",
    "    layer = Embedding(max_features,50,input_length=512)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(12,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.fit(embedding_matrix, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model Testing ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score on Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.concat([X_valid_Q1, y_valid_Q1.iloc[:,:12]], axis = 1)\n",
    "\n",
    "# pre-processing test data\n",
    "from tqdm import tqdm\n",
    "preprocessed_synopsis = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in df_valid['Comment'].values:\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_synopsis.append(sentance.strip())\n",
    "    \n",
    "    \n",
    "df_valid['preprocessed_comments']=preprocessed_synopsis\n",
    "\n",
    "# creating X and Y\n",
    "\n",
    "X_valid = df_valid[['preprocessed_comments']]\n",
    "y_valid = df_valid.drop(columns=['Comment', 'preprocessed_comments'])\n",
    "\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# creating padded dataset for x_valid\n",
    "encoded_docs_valid = vect.texts_to_sequences(X_valid['preprocessed_comments'])\n",
    "padded_docs_valid = pad_sequences(encoded_docs_valid, maxlen=max_len, padding='post')\n",
    "print(padded_docs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(padded_docs_valid,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/cnn_use_sub_themes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "pred = model.predict(padded_docs_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions=model.predict([padded_docs_test])\n",
    "predictions = pred\n",
    "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "\n",
    "for val in thresholds:\n",
    "    print(\"For threshold: \", val)\n",
    "    pred=predictions.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_valid, pred, average='micro')\n",
    "    recall = recall_score(y_valid, pred, average='micro')\n",
    "    f1 = f1_score(y_valid, pred, average='micro')\n",
    "   \n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
