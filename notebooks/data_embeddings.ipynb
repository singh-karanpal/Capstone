{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Embeddings\n",
    "### ...for the Advance Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadading\n",
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Preprocess\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Embeddings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  #TD-IDF & Bag of Words\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  #GloVe\n",
    "import tensorflow as tf  #Universal Sentence Encoder\n",
    "import tensorflow_hub as hub  #Universal Sentence Encoder\n",
    "from sentence_transformers import SentenceTransformer  #BERT\n",
    "\n",
    "# Models\n",
    "from keras.preprocessing.sequence import pad_sequences #Glove of CNN\n",
    "# from keras.models import Sequential #Glove of CNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling1D, MaxPool1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Input, Lambda\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score #Precision & Recall\n",
    "\n",
    "\n",
    "# To use GPU-Accelerated Machine Learning on MacOS\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Q1 = pd.read_csv('../data/X_train.csv')\n",
    "X_valid_Q1 = pd.read_csv('../data/X_valid.csv')\n",
    "\n",
    "y_train_Q1 = pd.read_csv('../data/y_train.csv')\n",
    "y_valid_Q1 = pd.read_csv('../data/y_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirement from BC Stats:$^{1}$\n",
    "\n",
    "| Level | Goal | Precision | Recall |\n",
    "|--------|------|-------------|-------|\n",
    "| Themes | primary (min) | 67% | 67% |\n",
    "| Themes | stretch (:tada:) | 80% | 80% |\n",
    "| | | |\n",
    "| Sub-themes | primary (min) | 50% | 50% |\n",
    "| Sub-themes | stretch (:tada:) | 67% | 67% |\n",
    "\n",
    "_(1) These standards were provided as a reference to shape the proposal of the project, but are not mandatory._\n",
    "\n",
    "_Note:  BC stats comment that they didnâ€™t include Accuracy as a key performance indicator (KPI) because it is so hard to gauge what a \"good\" result would be._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data for: GLOVE, BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the previous preprocess made by Sukriti & Vic\n",
    "X_train_Q1_pp1 = pd.read_csv('../data/X_train_pp.csv')\n",
    "X_valid_Q1_pp1 = pd.read_csv('../data/X_valid_pp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data for: USE and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_light(text, \n",
    "             irrelevant_pos = ['SPACE'],\n",
    "             avoid_entities = ['PERSON', 'ORG', 'LOC', 'GPE']):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, irrelevant_pos and avoid_entities, carries out \n",
    "    preprocessing of the text and returns list of preprocessed text. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (list) \n",
    "        the list of text to be preprocessed\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    avoid_entities : (list)\n",
    "        a list of entity labels to be avoided\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (list) list of preprocessed text\n",
    "    \n",
    "    Example\n",
    "    -------------\n",
    "    >>> example = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]\n",
    "\n",
    "    >>> preprocess(example)\n",
    "    (output:) [\"Hello, I 'm and I love swimming!\", \n",
    "               'I am a really good cook; what about you?',\n",
    "               'Contact me at']\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for sent in text:\n",
    "        sent = sent.lower()\n",
    "        sent = re.sub(r\"facebook\", \"social media\", sent)\n",
    "        sent = re.sub(r\"twitter\", \"social media\", sent)\n",
    "        sent = re.sub(r\"instagram\", \"social media\", sent)\n",
    "        sent = re.sub(r\"whatsapp\", \"social media\", sent)\n",
    "        sent = re.sub(r\"linkedin\", \"social media\", sent)\n",
    "        sent = re.sub(r\"snapchat\", \"social media\", sent)\n",
    "        \n",
    "        result_sent = []\n",
    "        doc = nlp(sent)\n",
    "        entities = [str(ent) for ent in doc.ents if ent.label_ in avoid_entities]\n",
    "        \n",
    "        for token in doc:            \n",
    "            if (token.like_email or\n",
    "                token.like_url or\n",
    "                token.pos_ in irrelevant_pos or\n",
    "                str(token) in entities):\n",
    "                continue\n",
    "            else:\n",
    "                if str(token) in string.punctuation:\n",
    "                    try:\n",
    "                        result_sent[-1] = str(result_sent[-1]) + str(token)\n",
    "                    except:\n",
    "                        result_sent.append(str(token))\n",
    "                else:\n",
    "                    result_sent.append(str(token))\n",
    "        result.append(\" \".join(result_sent))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Q1_pp2 = pp_light(X_train_Q1['Comment'])\n",
    "X_valid_Q1_pp2 = pp_light(X_valid_Q1['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_embed = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "t = time.time()\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train_Q1_pp1['Comment'].values.astype('U'))#\n",
    "X_valid_bow = vectorizer.transform(X_valid_Q1_pp1['Comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bow = time.time() - t\n",
    "case= {'Embedding': \"Bag of Words\",\n",
    "       'Time': time_bow}    \n",
    "times_embed.append(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-idf\n",
    "t = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_Q1_pp1['Comment'].values.astype('U'))#\n",
    "X_valid_tfidf = vectorizer.transform(X_valid_Q1_pp1['Comment'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tfidf = time.time() - t\n",
    "case= {'Embedding': \"TF-IDF\",\n",
    "       'Time': time_tfidf}    \n",
    "times_embed.append(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max lenght of comment: 87\n",
      "Vocabulary size: 8639\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "#Max length of sentence\n",
    "def max_len(x):\n",
    "    a=str(x).split()\n",
    "    return len(a)\n",
    "\n",
    "max_len = max(X_train_Q1_pp1['Comment'].apply(max_len))\n",
    "print(\"Max lenght of comment:\", max_len) \n",
    "\n",
    "# Vocab Size\n",
    "vect=Tokenizer()\n",
    "vect.fit_on_texts(X_train_Q1_pp1['Comment'].astype(str))\n",
    "vocab_size = len(vect.word_index) + 1\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole Glove embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/vcuspinera/Documents/UBC/B7_Capstone/Documents_capstone/11_Glove/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save which we won't see in Glove embedding\n",
    "not_in_glove = []\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix_GLOVE = np.zeros((vocab_size, 300))\n",
    "for word, i in vect.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_GLOVE[i] = embedding_vector\n",
    "    else:\n",
    "        not_in_glove.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8639, 300)"
      ]
     },
     "execution_count": 1099,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_GLOVE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_glove = time.time() - t\n",
    "case= {'Embedding': \"GloVe\",\n",
    "       'Time': time_glove}    \n",
    "times_embed.append(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe: Padding to make all sequences of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 485  532    5 ...    0    0    0]\n",
      " [ 108   82 1245 ...    0    0    0]\n",
      " [  24   42  265 ...    0    0    0]\n",
      " ...\n",
      " [ 396    8   23 ...    0    0    0]\n",
      " [ 255 2108 8637 ...    0    0    0]\n",
      " [ 361  538  596 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs_train = vect.texts_to_sequences(X_train_Q1_pp1['Comment'].astype(str))\n",
    "max_length = vocab_size\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_len, padding='post')\n",
    "print(padded_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  29   32 1740 ...    0    0    0]\n",
      " [   4  976   32 ...    0    0    0]\n",
      " [1176 4630  318 ...    0    0    0]\n",
      " ...\n",
      " [  52    1  109 ...    0    0    0]\n",
      " [  76  850  133 ...    0    0    0]\n",
      " [  26  436  445 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs_valid = vect.texts_to_sequences(X_valid_Q1_pp1['Comment'].astype(str))\n",
    "max_length = vocab_size\n",
    "padded_docs_valid = pad_sequences(encoded_docs_valid, maxlen=max_len, padding='post')\n",
    "print(padded_docs_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 87)"
      ]
     },
     "execution_count": 1103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the encoded information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the encoded of train and validatin datasets\n",
    "np.save(\"../data/embeddings/padded_docs_train\", padded_docs_train)\n",
    "np.save(\"../data/embeddings/padded_docs_valid\", padded_docs_valid)\n",
    "\n",
    "# saving the embeddings with GloVe\n",
    "np.save(\"../data/embeddings/embedding_matrix_GLOVE\", embedding_matrix_GLOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_USE_train = embed(X_train_Q1_pp2)#.astype(str).tolist())\n",
    "embeddings_USE_valid = embed(X_valid_Q1_pp2)#.astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10376, 512), dtype=float32, numpy=\n",
       "array([[-0.03905287, -0.07600362,  0.02124164, ...,  0.04003076,\n",
       "         0.00880638,  0.0135172 ],\n",
       "       [ 0.05754621,  0.02805524, -0.03187463, ...,  0.05339389,\n",
       "         0.06803529, -0.02249984],\n",
       "       [-0.00135626,  0.04400793, -0.01373925, ...,  0.02239931,\n",
       "        -0.03765979,  0.07158894],\n",
       "       ...,\n",
       "       [-0.02539974, -0.05343602,  0.0577434 , ...,  0.01711884,\n",
       "         0.01079335, -0.00586297],\n",
       "       [-0.03075003, -0.04403489,  0.01170156, ...,  0.07016593,\n",
       "        -0.05456427,  0.0272686 ],\n",
       "       [ 0.02614759, -0.05074428,  0.00971   , ...,  0.01864268,\n",
       "         0.05439513,  0.03895477]], dtype=float32)>"
      ]
     },
     "execution_count": 1107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_USE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_USE_train = np.array(embeddings_USE_train)\n",
    "embedding_matrix_USE_valid = np.array(embeddings_USE_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_use = time.time() - t\n",
    "case= {'Embedding': \"Univ. Sentence Encoder\",\n",
    "       'Time': time_use}    \n",
    "times_embed.append(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03905287, -0.07600362,  0.02124164, ...,  0.04003076,\n",
       "         0.00880638,  0.0135172 ],\n",
       "       [ 0.05754621,  0.02805524, -0.03187463, ...,  0.05339389,\n",
       "         0.06803529, -0.02249984],\n",
       "       [-0.00135626,  0.04400793, -0.01373925, ...,  0.02239931,\n",
       "        -0.03765979,  0.07158894],\n",
       "       ...,\n",
       "       [-0.02539974, -0.05343602,  0.0577434 , ...,  0.01711884,\n",
       "         0.01079335, -0.00586297],\n",
       "       [-0.03075003, -0.04403489,  0.01170156, ...,  0.07016593,\n",
       "        -0.05456427,  0.0272686 ],\n",
       "       [ 0.02614759, -0.05074428,  0.00971   , ...,  0.01864268,\n",
       "         0.05439513,  0.03895477]], dtype=float32)"
      ]
     },
     "execution_count": 1110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_USE_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the encoded information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the embeddings with USE\n",
    "np.save(\"../data/embeddings/embedding_matrix_USE_train\", embedding_matrix_USE_train)\n",
    "np.save(\"../data/embeddings/embedding_matrix_USE_valid\", embedding_matrix_USE_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "t = time.time()\n",
    "bert_encoder = SentenceTransformer('bert-large-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_BERT_train = bert_encoder.encode(X_train_Q1_pp2)\n",
    "embeddings_BERT_valid = bert_encoder.encode(X_valid_Q1_pp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Themes - Vectorize preprocessed X train pre-processed dataset\n",
    "embedding_matrix_BERT_train = np.asarray(embeddings_BERT_train)\n",
    "embedding_matrix_BERT_valid = np.asarray(embeddings_BERT_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bert = time.time() - t\n",
    "case= {'Embedding': \"BERT\",\n",
    "       'Time': time_bert}    \n",
    "times_embed.append(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10376, 1024)"
      ]
     },
     "execution_count": 1007,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_BERT_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4034353 , -0.03421297,  0.6793261 , ..., -0.97726643,\n",
       "        -0.38727477, -0.11952855],\n",
       "       [ 0.44806713,  0.3930342 ,  0.5015124 , ..., -0.3754626 ,\n",
       "        -0.36841637, -0.25009534],\n",
       "       [ 0.4140735 , -0.41724584,  0.50169885, ...,  0.05167035,\n",
       "        -0.11021139, -0.57716227],\n",
       "       ...,\n",
       "       [ 0.50822234, -0.2311309 ,  0.4302179 , ..., -0.7286546 ,\n",
       "        -0.24566731,  0.6600919 ],\n",
       "       [ 0.28324887, -0.0622227 ,  0.19700417, ..., -0.6093314 ,\n",
       "        -0.21683584,  0.1300633 ],\n",
       "       [-0.23951472, -0.00656399,  0.4725369 , ..., -0.02723284,\n",
       "        -1.3340077 , -0.06195428]], dtype=float32)"
      ]
     },
     "execution_count": 1008,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_BERT_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the encoded information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the embeddings with BERT\n",
    "np.save(\"../data/embeddings/embedding_matrix_BERT_train\", embedding_matrix_BERT_train)\n",
    "np.save(\"../data/embeddings/embedding_matrix_BERT_valid\", embedding_matrix_BERT_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Times for embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.382224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.368383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe</td>\n",
       "      <td>28.344412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Univ. Sentence Encoder</td>\n",
       "      <td>10.390384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>2619.691087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Embedding         Time\n",
       "0            Bag of Words     0.382224\n",
       "1                  TF-IDF     0.368383\n",
       "2                   GloVe    28.344412\n",
       "3  Univ. Sentence Encoder    10.390384\n",
       "4                    BERT  2619.691087"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(times_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `y` as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert all y to an array\n",
    "# y_train = (np.array(y_train_Q1))\n",
    "# y_valid = (np.array(y_valid_Q1))\n",
    "\n",
    "# ### DO WE NEED THIS ONE??? I THINK WE DON'T USE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `y` by themes and sub-themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme columns: 12 , shape: (10376, 12)\n",
      "Subtheme columns: 62 , shape: (10376, 62)\n"
     ]
    }
   ],
   "source": [
    "# Slice `y` to themes and subthemes.\n",
    "# Note: we never use 'Unrelated' as Theme netiher Sub-theme\n",
    "\n",
    "#y_train\n",
    "y_train_thm = y_train_Q1.loc[:, 'CPD':'OTH']\n",
    "y_train_sub = y_train_Q1.loc[:, 'CPD_Improve_new_employee_orientation':'OTH_Covid']\n",
    "\n",
    "#y_valid\n",
    "y_valid_thm = y_valid_Q1.loc[:, 'CPD':'OTH']\n",
    "y_valid_sub = y_valid_Q1.loc[:, 'CPD_Improve_new_employee_orientation':'OTH_Covid']\n",
    "\n",
    "\n",
    "# y's as arrays\n",
    "y_train_thm = (np.array(y_train_thm))\n",
    "y_train_sub = (np.array(y_train_sub))\n",
    "y_valid_thm = (np.array(y_valid_thm))\n",
    "y_valid_sub = (np.array(y_valid_sub))\n",
    "\n",
    "#shape check: 13 themes and 62 subthemes\n",
    "print('Theme columns:',y_train_thm.shape[1], \", shape:\", np.shape(y_train_thm))\n",
    "print('Subtheme columns:', y_train_sub.shape[1], \", shape:\", np.shape(y_train_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving `y` as numpy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving THEME and SUB-THEME targets\n",
    "np.save(\"../data/embeddings/y_train_thm\", y_train_thm)\n",
    "np.save(\"../data/embeddings/y_train_sub\", y_train_sub)\n",
    "np.save(\"../data/embeddings/y_valid_thm\", y_valid_thm)\n",
    "np.save(\"../data/embeddings/y_valid_sub\", y_valid_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "###                                                      ###\n",
    "###   SECTION WITH BASIC MODELS, DON'T RUN AFTER THIS.   ###\n",
    "###                                                      ###\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling with CNN - only for themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Glove + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix_GLOVE.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 300 # for glove we are using 300d dataset\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 87, 300)           2591700   \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 87, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 85, 250)           225250    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_87 (MaxPooling (None, 42, 250)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_85 (Conv1D)           (None, 40, 250)           187750    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 20, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 250)               1250250   \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_192 (Dense)            (None, 12)                3012      \n",
      "=================================================================\n",
      "Total params: 4,257,962\n",
      "Trainable params: 1,666,262\n",
      "Non-trainable params: 2,591,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Embedding(max_features, embed_size, weights=[embedding_matrix_GLOVE],\n",
    "                    trainable=False, input_length=maxlen))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',\n",
    "                 strides=1))\n",
    "model1.add(MaxPooling1D())\n",
    "model1.add(Conv1D(filters, kernel_size, padding='valid',activation='relu'))\n",
    "model1.add(MaxPooling1D())\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(hidden_dims, activation = 'relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(n_class, activation = 'sigmoid'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8819 samples, validate on 1557 samples\n",
      "8819/8819 [==============================] - 8s 959us/sample - loss: 0.3616 - accuracy: 0.8811 - val_loss: 0.2972 - val_accuracy: 0.8949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1aaa790a90>"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model1.fit(padded_docs_train, y_train_thm, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 1s 257us/sample - loss: 0.2939 - accuracy: 0.8960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2939400156369647, 0.8960099]"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "# encoded_docs_valid = vect.texts_to_sequences(X_valid_Q1['Comment'])\n",
    "# padded_docs_valid = pad_sequences(encoded_docs_valid, maxlen=max_len, padding='post')\n",
    "\n",
    "score = model1.evaluate(padded_docs_valid, y_valid_thm)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 1s 215us/sample\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(padded_docs_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13142534, 0.12893617, 0.12386531, ..., 0.04853222, 0.1706842 ,\n",
       "        0.04150677],\n",
       "       [0.12033603, 0.19773522, 0.09779354, ..., 0.0637963 , 0.13004135,\n",
       "        0.04049535],\n",
       "       [0.04034825, 0.01286739, 0.02769243, ..., 0.93513274, 0.06735138,\n",
       "        0.02334217],\n",
       "       ...,\n",
       "       [0.18681641, 0.16545677, 0.10535166, ..., 0.1027528 , 0.16285451,\n",
       "        0.03809162],\n",
       "       [0.15256792, 0.3459331 , 0.07776302, ..., 0.06837911, 0.18199798,\n",
       "        0.02498189],\n",
       "       [0.06668898, 0.08049981, 0.06052296, ..., 0.35278776, 0.08763526,\n",
       "        0.04004364]], dtype=float32)"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.116230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>0.964898</td>\n",
       "      <td>0.252413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.185112</td>\n",
       "      <td>0.877004</td>\n",
       "      <td>0.305699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.692095</td>\n",
       "      <td>0.367965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.358316</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.406788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.502216</td>\n",
       "      <td>0.313156</td>\n",
       "      <td>0.385768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.672457</td>\n",
       "      <td>0.224710</td>\n",
       "      <td>0.336855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.787289</td>\n",
       "      <td>0.167772</td>\n",
       "      <td>0.276601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.847682</td>\n",
       "      <td>0.141515</td>\n",
       "      <td>0.242539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.130459</td>\n",
       "      <td>0.226814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.878728</td>\n",
       "      <td>0.122167</td>\n",
       "      <td>0.214511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.899358</td>\n",
       "      <td>0.116086</td>\n",
       "      <td>0.205630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.197336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.922705</td>\n",
       "      <td>0.105583</td>\n",
       "      <td>0.189484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.095080</td>\n",
       "      <td>0.172431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.088447</td>\n",
       "      <td>0.161535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.939490</td>\n",
       "      <td>0.081537</td>\n",
       "      <td>0.150051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.948529</td>\n",
       "      <td>0.071310</td>\n",
       "      <td>0.132648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.053897</td>\n",
       "      <td>0.102121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.059501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Precision    Recall  F1-measure\n",
       "0        0.00   0.116230  1.000000    0.208254\n",
       "1        0.05   0.145198  0.964898    0.252413\n",
       "2        0.10   0.185112  0.877004    0.305699\n",
       "3        0.15   0.250600  0.692095    0.367965\n",
       "4        0.20   0.358316  0.470426    0.406788\n",
       "5        0.25   0.502216  0.313156    0.385768\n",
       "6        0.30   0.672457  0.224710    0.336855\n",
       "7        0.35   0.787289  0.167772    0.276601\n",
       "8        0.40   0.847682  0.141515    0.242539\n",
       "9        0.45   0.867647  0.130459    0.226814\n",
       "10       0.50   0.878728  0.122167    0.214511\n",
       "11       0.55   0.899358  0.116086    0.205630\n",
       "12       0.60   0.917431  0.110558    0.197336\n",
       "13       0.65   0.922705  0.105583    0.189484\n",
       "14       0.70   0.924731  0.095080    0.172431\n",
       "15       0.75   0.930233  0.088447    0.161535\n",
       "16       0.80   0.939490  0.081537    0.150051\n",
       "17       0.85   0.948529  0.071310    0.132648\n",
       "18       0.90   0.970149  0.053897    0.102121\n",
       "19       0.95   0.982301  0.030680    0.059501"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_glove = []\n",
    "thresholds=np.arange(0, 1, 0.05).tolist()\n",
    "\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(y_valid_thm, pred, average='micro')\n",
    "    recall = recall_score(y_valid_thm, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_thm, pred, average='micro')\n",
    "   \n",
    "    case= {'Threshold': val,\n",
    "           'Precision': precision,\n",
    "           'Recall': recall,\n",
    "           'F1-measure': f1}\n",
    "    predictions_glove.append(case)\n",
    "\n",
    "print(\"Micro-average quality numbers:\")\n",
    "pd.DataFrame(predictions_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: USE + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix_USE_train.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 512 # for universal sentence encoder\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the EMBEDDING MATRIX: (10376, 512)\n",
      "Shape of SIMILARITY MATRIX: (10376, 10376)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.99999994,  0.03254884,  0.02044772, ...,  0.19620545,\n",
       "         0.18911791,  0.02506309],\n",
       "       [ 0.03254884,  0.99999976,  0.06175764, ...,  0.04994562,\n",
       "         0.08648334, -0.03285146],\n",
       "       [ 0.02044772,  0.06175764,  0.9999999 , ...,  0.11044698,\n",
       "         0.0220431 ,  0.08513585],\n",
       "       ...,\n",
       "       [ 0.19620545,  0.04994562,  0.11044698, ...,  0.99999976,\n",
       "         0.12881677,  0.04329847],\n",
       "       [ 0.18911791,  0.08648334,  0.0220431 , ...,  0.12881677,\n",
       "         1.0000004 ,  0.01171911],\n",
       "       [ 0.02506309, -0.03285146,  0.08513585, ...,  0.04329847,\n",
       "         0.01171911,  0.9999997 ]], dtype=float32)"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since the values are normalized, the inner product of encodings\n",
    "# can be treated as a SIMILARITY MATRIX. This show us the similarity\n",
    "# between comments.\n",
    "print(\"Shape of the EMBEDDING MATRIX:\", embedding_matrix_USE_train.shape)\n",
    "similarity_matrix_USE = np.inner(embedding_matrix_USE_train, embedding_matrix_USE_train)\n",
    "print(\"Shape of SIMILARITY MATRIX:\", similarity_matrix_USE.shape)\n",
    "similarity_matrix_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TOY EXAMPLE 1\n",
    "# model2 = Sequential()\n",
    "# model2.add(Dense(128, activation = 'relu'))\n",
    "# model2.add(Dense(n_class, activation = 'softmax'))\n",
    "\n",
    "# model2.build((None, max_features, embed_size))\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TOY EXAMPLE 2\n",
    "# model2 = Sequential()\n",
    "# model2.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "# model2.add(Dropout(0.2))\n",
    "# # model2.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',\n",
    "# #                  strides=1))\n",
    "# # model2.add(MaxPooling1D())\n",
    "# # model2.add(Conv1D(filters, kernel_size, padding='valid',activation='relu'))\n",
    "# # model2.add(MaxPooling1D())\n",
    "# # model2.add(Flatten())\n",
    "# model2.add(Dense(hidden_dims, activation = 'relu'))\n",
    "# model2.add(Dropout(0.5))\n",
    "# model2.add(Dense(n_class, activation = 'softmax'))\n",
    "\n",
    "# model2.build((None, max_features, embed_size))\n",
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_193 (Dense)            (None, 10376)             5322888   \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dense_194 (Dense)            (None, 12)                124524    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 5,447,412\n",
      "Trainable params: 5,447,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TOY EXAMPLE 3\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(max_features, input_shape=(embed_size,)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(n_class))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8819 samples, validate on 1557 samples\n",
      "8819/8819 [==============================] - 4s 409us/sample - loss: 0.2899 - accuracy: 0.8988 - val_loss: 0.2058 - val_accuracy: 0.9236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a828a7690>"
      ]
     },
     "execution_count": 1027,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "model2.fit(embedding_matrix_USE_train, y_train_thm, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 0s 95us/sample - loss: 0.2009 - accuracy: 0.9243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20086015897617032, 0.9242805]"
      ]
     },
     "execution_count": 1028,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model2.evaluate(embedding_matrix_USE_valid, y_valid_thm)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 0s 52us/sample\n"
     ]
    }
   ],
   "source": [
    "predictions = model2.predict(embedding_matrix_USE_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06698422, 0.00153247, 0.00859099, ..., 0.00305651, 0.3634413 ,\n",
       "        0.02548531],\n",
       "       [0.01808216, 0.00555592, 0.23882768, ..., 0.01634414, 0.0769001 ,\n",
       "        0.05658474],\n",
       "       [0.03724623, 0.01348698, 0.09376555, ..., 0.9153991 , 0.05189299,\n",
       "        0.15749778],\n",
       "       ...,\n",
       "       [0.0523813 , 0.10979683, 0.03114   , ..., 0.17261714, 0.01866454,\n",
       "        0.04359911],\n",
       "       [0.11188969, 0.954266  , 0.02390652, ..., 0.01514343, 0.01504079,\n",
       "        0.02143048],\n",
       "       [0.09486086, 0.00264859, 0.01919152, ..., 0.0058036 , 0.24384539,\n",
       "        0.0464747 ]], dtype=float32)"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-average quality numbers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.116230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.266344</td>\n",
       "      <td>0.927861</td>\n",
       "      <td>0.413882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.376007</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.521673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.456157</td>\n",
       "      <td>0.775014</td>\n",
       "      <td>0.574296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.523263</td>\n",
       "      <td>0.705638</td>\n",
       "      <td>0.600918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.578570</td>\n",
       "      <td>0.646213</td>\n",
       "      <td>0.610524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.633960</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.614640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.686051</td>\n",
       "      <td>0.547816</td>\n",
       "      <td>0.609190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.723817</td>\n",
       "      <td>0.494748</td>\n",
       "      <td>0.587752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.763405</td>\n",
       "      <td>0.448590</td>\n",
       "      <td>0.565111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.794995</td>\n",
       "      <td>0.412659</td>\n",
       "      <td>0.543304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.828763</td>\n",
       "      <td>0.375898</td>\n",
       "      <td>0.517209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.853793</td>\n",
       "      <td>0.342178</td>\n",
       "      <td>0.488556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.870140</td>\n",
       "      <td>0.309287</td>\n",
       "      <td>0.456362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.892826</td>\n",
       "      <td>0.278607</td>\n",
       "      <td>0.424689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.910626</td>\n",
       "      <td>0.253455</td>\n",
       "      <td>0.396541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.920314</td>\n",
       "      <td>0.226645</td>\n",
       "      <td>0.363717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.936913</td>\n",
       "      <td>0.192924</td>\n",
       "      <td>0.319963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.955738</td>\n",
       "      <td>0.161139</td>\n",
       "      <td>0.275781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.982103</td>\n",
       "      <td>0.121338</td>\n",
       "      <td>0.215990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Precision    Recall  F1-measure\n",
       "0        0.00   0.116230  1.000000    0.208254\n",
       "1        0.05   0.266344  0.927861    0.413882\n",
       "2        0.10   0.376007  0.851575    0.521673\n",
       "3        0.15   0.456157  0.775014    0.574296\n",
       "4        0.20   0.523263  0.705638    0.600918\n",
       "5        0.25   0.578570  0.646213    0.610524\n",
       "6        0.30   0.633960  0.596462    0.614640\n",
       "7        0.35   0.686051  0.547816    0.609190\n",
       "8        0.40   0.723817  0.494748    0.587752\n",
       "9        0.45   0.763405  0.448590    0.565111\n",
       "10       0.50   0.794995  0.412659    0.543304\n",
       "11       0.55   0.828763  0.375898    0.517209\n",
       "12       0.60   0.853793  0.342178    0.488556\n",
       "13       0.65   0.870140  0.309287    0.456362\n",
       "14       0.70   0.892826  0.278607    0.424689\n",
       "15       0.75   0.910626  0.253455    0.396541\n",
       "16       0.80   0.920314  0.226645    0.363717\n",
       "17       0.85   0.936913  0.192924    0.319963\n",
       "18       0.90   0.955738  0.161139    0.275781\n",
       "19       0.95   0.982103  0.121338    0.215990"
      ]
     },
     "execution_count": 1051,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_use = []\n",
    "thresholds=np.arange(0, 1, 0.05).tolist()\n",
    "\n",
    "model2.build((None, max_features, embed_size))\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(y_valid_thm, pred, average='micro')\n",
    "    recall = recall_score(y_valid_thm, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_thm, pred, average='micro')\n",
    "   \n",
    "    case= {'Threshold': val,\n",
    "           'Precision': precision,\n",
    "           'Recall': recall,\n",
    "           'F1-measure': f1}\n",
    "    predictions_use.append(case)\n",
    "\n",
    "print(\"\\nMicro-average quality numbers:\")\n",
    "pd.DataFrame(predictions_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: BERT + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix_BERT_train.shape[0]\n",
    "maxlen = max_len\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 1\n",
    "embed_size = 1024 # for BERT Large\n",
    "n_class = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_197 (Dense)            (None, 10376)             10635400  \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 10376)             0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 12)                124524    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 10,759,924\n",
      "Trainable params: 10,759,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model3 = Sequential()\n",
    "# # model3.add(Embedding(max_features, embed_size, #weights=[embedding_matrix_USE],\n",
    "# #                         trainable=False, input_length=embed_size))#maxlen))\n",
    "# #model3.add(Dropout(0.2))\n",
    "# model3.add(Conv1D(filters, kernel_size, padding='valid', activation='relu',\n",
    "#                  strides=1))\n",
    "# model3.add(MaxPooling1D())\n",
    "# model3.add(Conv1D(filters, kernel_size, padding='valid',activation='relu'))\n",
    "# model3.add(MaxPooling1D())\n",
    "# model3.add(Flatten())\n",
    "# model3.add(Dense(hidden_dims, activation = 'relu'))\n",
    "# model3.add(Dropout(0.5))\n",
    "# model3.add(Dense(n_class, activation = 'sigmoid'))\n",
    "# model3.summary()\n",
    "\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(max_features, input_shape=(embed_size,)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(n_class))\n",
    "model3.add(Activation('sigmoid'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8819 samples, validate on 1557 samples\n",
      "8819/8819 [==============================] - 6s 651us/sample - loss: 0.2767 - accuracy: 0.9022 - val_loss: 0.2195 - val_accuracy: 0.9166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a82da0910>"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Model\n",
    "model3.fit(embedding_matrix_BERT_train, y_train_thm, batch_size=batch_size, epochs=epochs,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 1s 314us/sample - loss: 0.2108 - accuracy: 0.9196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2108168209986955, 0.919558]"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model3.evaluate(embedding_matrix_BERT_valid, y_valid_thm)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25941345,  0.36210942,  0.49052876, ..., -0.05945268,\n",
       "        -0.59524155, -0.42596203],\n",
       "       [ 0.12129721, -0.91398495,  0.38285968, ..., -0.02158303,\n",
       "        -0.81365967, -0.6090022 ],\n",
       "       [ 0.03514948,  0.1167607 ,  0.02573913, ..., -0.64796126,\n",
       "        -0.8018599 , -0.10790344],\n",
       "       ...,\n",
       "       [ 0.55087906,  0.22932735,  0.298204  , ..., -0.34927025,\n",
       "        -0.6513062 , -0.21662723],\n",
       "       [ 0.59616655,  0.86357677,  0.5159346 , ..., -0.25301898,\n",
       "        -0.12649104, -0.02165654],\n",
       "       [ 0.13343003, -0.38848475,  0.1278247 , ..., -0.4058949 ,\n",
       "        -0.1268155 , -0.28904745]], dtype=float32)"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_BERT_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 0s 99us/sample\n"
     ]
    }
   ],
   "source": [
    "predictions = model3.predict(embedding_matrix_BERT_valid, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Micro-average quality numbers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.116230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.208254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.258060</td>\n",
       "      <td>0.927032</td>\n",
       "      <td>0.403732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.360747</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.505422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.773355</td>\n",
       "      <td>0.560104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.509683</td>\n",
       "      <td>0.705638</td>\n",
       "      <td>0.591863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.564066</td>\n",
       "      <td>0.647319</td>\n",
       "      <td>0.602831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.610538</td>\n",
       "      <td>0.605307</td>\n",
       "      <td>0.607911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.659734</td>\n",
       "      <td>0.561083</td>\n",
       "      <td>0.606423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.698666</td>\n",
       "      <td>0.521006</td>\n",
       "      <td>0.596897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.724801</td>\n",
       "      <td>0.478994</td>\n",
       "      <td>0.576801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.767788</td>\n",
       "      <td>0.441404</td>\n",
       "      <td>0.560548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.804204</td>\n",
       "      <td>0.401879</td>\n",
       "      <td>0.535938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.828733</td>\n",
       "      <td>0.365119</td>\n",
       "      <td>0.506907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.853004</td>\n",
       "      <td>0.333610</td>\n",
       "      <td>0.479634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.869808</td>\n",
       "      <td>0.300995</td>\n",
       "      <td>0.447228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.890511</td>\n",
       "      <td>0.269762</td>\n",
       "      <td>0.414086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.378449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.930643</td>\n",
       "      <td>0.203980</td>\n",
       "      <td>0.334618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.952160</td>\n",
       "      <td>0.170536</td>\n",
       "      <td>0.289264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>0.224835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Precision    Recall  F1-measure\n",
       "0        0.00   0.116230  1.000000    0.208254\n",
       "1        0.05   0.258060  0.927032    0.403732\n",
       "2        0.10   0.360747  0.843836    0.505422\n",
       "3        0.15   0.439040  0.773355    0.560104\n",
       "4        0.20   0.509683  0.705638    0.591863\n",
       "5        0.25   0.564066  0.647319    0.602831\n",
       "6        0.30   0.610538  0.605307    0.607911\n",
       "7        0.35   0.659734  0.561083    0.606423\n",
       "8        0.40   0.698666  0.521006    0.596897\n",
       "9        0.45   0.724801  0.478994    0.576801\n",
       "10       0.50   0.767788  0.441404    0.560548\n",
       "11       0.55   0.804204  0.401879    0.535938\n",
       "12       0.60   0.828733  0.365119    0.506907\n",
       "13       0.65   0.853004  0.333610    0.479634\n",
       "14       0.70   0.869808  0.300995    0.447228\n",
       "15       0.75   0.890511  0.269762    0.414086\n",
       "16       0.80   0.911392  0.238806    0.378449\n",
       "17       0.85   0.930643  0.203980    0.334618\n",
       "18       0.90   0.952160  0.170536    0.289264\n",
       "19       0.95   0.987097  0.126866    0.224835"
      ]
     },
     "execution_count": 1059,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_bert = []\n",
    "thresholds=np.arange(0, 1, 0.05).tolist()\n",
    "\n",
    "for val in thresholds:\n",
    "    pred=predictions.copy()\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    precision = precision_score(y_valid_thm, pred, average='micro')\n",
    "    recall = recall_score(y_valid_thm, pred, average='micro')\n",
    "    f1 = f1_score(y_valid_thm, pred, average='micro')\n",
    "   \n",
    "    case= {'Threshold': val,\n",
    "           'Precision': precision,\n",
    "           'Recall': recall,\n",
    "           'F1-measure': f1}\n",
    "    predictions_bert.append(case)\n",
    "\n",
    "print(\"\\nMicro-average quality numbers:\")\n",
    "pd.DataFrame(predictions_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE + CNN -> Varada's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module https://tfhub.dev/google/universal-sentence-encoder/4 loaded\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "# import seaborn as sns\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
    "model = hub.load(module_url)\n",
    "print (\"module %s loaded\" % module_url)\n",
    "def embed(input):\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load comments\n",
    "X_train = pd.read_csv('../data/X_train.csv')['Comment'].to_numpy()\n",
    "X_valid = pd.read_csv('../data/X_valid.csv')['Comment'].to_numpy()\n",
    "\n",
    "# load labels\n",
    "y_train = pd.read_csv('../data/y_train.csv')\n",
    "y_valid = pd.read_csv('../data/y_valid.csv')\n",
    "# select only themes' labels\n",
    "y_train = y_train.loc[:, 'CPD':'OTH'].to_numpy()\n",
    "y_valid = y_valid.loc[:, 'CPD':'OTH'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 10376, test entries: 2594\n"
     ]
    }
   ],
   "source": [
    "print(\"Training entries: {}, test entries: {}\".format(len(X_train), len(X_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train    y_train\n",
      " (10376,) (10376, 12) \n",
      "\n",
      "X_valid    y_valid\n",
      " (2594,) (2594, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train   \", \"y_train\\n\",\n",
    "      X_train.shape,\n",
    "      y_train.shape,\n",
    "      \"\\n\\nX_valid   \", \"y_valid\\n\",\n",
    "      X_valid.shape,\n",
    "      y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['to be real about diversity, you need to create seats at the table. That means affirmative action type programs.  We still see men advancing in leadership at MCFD, then white women, people of color are left behind and have token representation ',\n",
       "       'Keep the building warmer and provide warm water in the bathroom.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]\n",
    "# type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use embeddings given by universal sentence encoder \n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "hub_layer = hub.KerasLayer(model, output_shape=[512], input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 512])"
      ]
     },
     "execution_count": 1163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_layer(X_train[:3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           [(None,)]                 0         \n",
      "_________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)   (None, 512)               256797824 \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 512, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_92 (Conv1D)           (None, 512, 128)          384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_95 (MaxPooling (None, 103, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_93 (Conv1D)           (None, 103, 128)          49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_96 (MaxPooling (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_94 (Conv1D)           (None, 21, 128)           65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_97 (MaxPooling (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 256,931,212\n",
      "Trainable params: 256,931,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN on the top of USE embeddings. The difference\n",
    "# with Varada model is that we already will give the embeddings.\n",
    "input = Input(shape=(), name=\"Input\", dtype=tf.string)\n",
    "x = hub_layer(input)\n",
    "x = tf.keras.layers.Reshape(input_shape=(512,), target_shape=(512, 1))(x)\n",
    "x = tf.keras.layers.Conv1D(128, 2, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(5, padding='same')(x)\n",
    "x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(5, padding='same')(x)\n",
    "x = tf.keras.layers.Conv1D(128, 4, activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling1D(40, padding='same')(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(12, activation='sigmoid')(x)\n",
    "m = Model(input, output)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(optimizer='adam',\n",
    "              loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10376 samples, validate on 2594 samples\n",
      "Epoch 1/20\n",
      "10376/10376 [==============================] - 51s 5ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 2/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 3/20\n",
      "10376/10376 [==============================] - 47s 5ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 4/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 5/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 6/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 7/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 8/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 9/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 10/20\n",
      "10376/10376 [==============================] - 47s 5ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 11/20\n",
      "10376/10376 [==============================] - 47s 5ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 12/20\n",
      "10376/10376 [==============================] - 47s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 13/20\n",
      "10376/10376 [==============================] - 47s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 14/20\n",
      "10376/10376 [==============================] - 47s 5ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 15/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 16/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 17/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 18/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 19/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n",
      "Epoch 20/20\n",
      "10376/10376 [==============================] - 46s 4ms/sample - loss: 0.6931 - accuracy: 0.8830 - val_loss: 0.6931 - val_accuracy: 0.8838\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(X_train,\n",
    "                y_train,\n",
    "                epochs=10, #20,\n",
    "                batch_size=512,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-f86d51dfbba54bd2b36ef71678fd81c6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-f86d51dfbba54bd2b36ef71678fd81c6\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-b640a4cc6f9eccd712c70e3a78a74038\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"metric\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"epoch\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"value\"}}, \"title\": \"Loss and Accuracy\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-b640a4cc6f9eccd712c70e3a78a74038\": [{\"epoch\": 0, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 0, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 0, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 0, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 1, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 1, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 1, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 1, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 2, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 2, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 2, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 2, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 3, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 3, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 3, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 3, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 4, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 4, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 4, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 4, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 5, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 5, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 5, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 5, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 6, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 6, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 6, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 6, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 7, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 7, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 7, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 7, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 8, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 8, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 8, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 8, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 9, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 9, \"metric\": \"train_accuracy\", \"value\": 0.8830234408378601}, {\"epoch\": 9, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 9, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 10, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 10, \"metric\": \"train_accuracy\", \"value\": 0.8830234408378601}, {\"epoch\": 10, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 10, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 11, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 11, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 11, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 11, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 12, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 12, \"metric\": \"train_accuracy\", \"value\": 0.8830234408378601}, {\"epoch\": 12, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 12, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 13, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 13, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 13, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 13, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 14, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 14, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 14, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 14, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 15, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 15, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 15, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 15, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 16, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 16, \"metric\": \"train_accuracy\", \"value\": 0.8830235004425049}, {\"epoch\": 16, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 16, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 17, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 17, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 17, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 17, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 18, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 18, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 18, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 18, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}, {\"epoch\": 19, \"metric\": \"train_loss\", \"value\": 0.693147066380303}, {\"epoch\": 19, \"metric\": \"train_accuracy\", \"value\": 0.8830236196517944}, {\"epoch\": 19, \"metric\": \"valid_loss\", \"value\": 0.6931470648178065}, {\"epoch\": 19, \"metric\": \"valid_accuracy\", \"value\": 0.8837704062461853}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot\n",
    "import altair as alt\n",
    "# alt.renderers.enable('default')\n",
    "# alt.data_transformers.enable('json')\n",
    "\n",
    "# funtion to plot\n",
    "def plot_metrics(hist):\n",
    "    '''\n",
    "    Returns an Altair plot of the loss and accuracy for the train and \n",
    "    validation datasets based in the history of the model\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    hist (object) tensorflow.python.keras.callbacks.History\n",
    "\n",
    "    Output:\n",
    "    -------\n",
    "    Altair plot\n",
    "    '''\n",
    "    df = pd.DataFrame(hist.history.values(), hist.history.keys())\\\n",
    "        .T.rename(columns={\"loss\":\"train_loss\",\n",
    "                          \"accuracy\":\"train_accuracy\",\n",
    "                          \"val_loss\":\"valid_loss\",\n",
    "                          \"val_accuracy\":\"valid_accuracy\"})\n",
    "    df = pd.DataFrame(df.stack()).reset_index().drop(columns=[])\\\n",
    "        .rename(columns={\"level_0\":'epoch', 'level_1':'metric', 0:'value'})\n",
    "    plot = alt.Chart(df).mark_line().encode(\n",
    "        x='epoch:Q',\n",
    "        y='value:Q',\n",
    "        color='metric'\n",
    "    ).properties(\n",
    "        title='Loss and Accuracy'\n",
    "    )\n",
    "    return plot\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594/2594 [==============================] - 2s 596us/sample\n",
      "Micro-average quality numbers:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy all model</th>\n",
       "      <th>Accuracy average (keras)</th>\n",
       "      <th>Hamming loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.88377</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.88377</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.88377</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.88377</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.88377</td>\n",
       "      <td>0.11623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold  Accuracy all model  Accuracy average (keras)  Hamming loss  \\\n",
       "0        0.5            0.002699                   0.88377       0.11623   \n",
       "1        0.6            0.002699                   0.88377       0.11623   \n",
       "2        0.7            0.002699                   0.88377       0.11623   \n",
       "3        0.8            0.002699                   0.88377       0.11623   \n",
       "4        0.9            0.002699                   0.88377       0.11623   \n",
       "\n",
       "   Precision  Recall  F1-measure  \n",
       "0        0.0     0.0         0.0  \n",
       "1        0.0     0.0         0.0  \n",
       "2        0.0     0.0         0.0  \n",
       "3        0.0     0.0         0.0  \n",
       "4        0.0     0.0         0.0  "
      ]
     },
     "execution_count": 1171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HYPERPARAMS\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, hamming_loss\n",
    "\n",
    "max_features = X_train.shape[0] # comments\n",
    "batch_size = 128\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 20\n",
    "embed_size = 512 # for USE\n",
    "n_class = 12 # 12 for themes and 62 for sub-themes\n",
    "\n",
    "# PREDICTIONS\n",
    "# get the prediction\n",
    "y_pred = m.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "# rounding the predictions\n",
    "y_pred_binary = (y_pred > 0.5) * 1\n",
    "\n",
    "\n",
    "# PRECISION & RECALL\n",
    "predictions_results = []\n",
    "thresholds=np.arange(.5, 1, 0.1).tolist()\n",
    "\n",
    "for val in thresholds:\n",
    "    pred = y_pred.copy()\n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "\n",
    "    accuracy = accuracy_score(y_valid, pred, normalize=True, sample_weight=None)\n",
    "    res = []\n",
    "    for i in range(0, y_valid.shape[1]):\n",
    "        res.append(accuracy_score(y_valid[:,i], y_pred_binary[:,i]))\n",
    "    accuracy_keras = np.mean(res)\n",
    "    hamming = hamming_loss(y_valid, pred)\n",
    "    precision = precision_score(y_valid, pred, average='micro')\n",
    "    recall = recall_score(y_valid, pred, average='micro')\n",
    "    f1 = f1_score(y_valid, pred, average='micro')\n",
    "   \n",
    "    case= {'Threshold': val,\n",
    "           'Accuracy all model': accuracy,\n",
    "           'Accuracy average (keras)': accuracy_keras,\n",
    "           'Hamming loss': hamming,\n",
    "           'Precision': precision,\n",
    "           'Recall': recall,\n",
    "           'F1-measure': f1}\n",
    "    predictions_results.append(case)\n",
    "\n",
    "print(\"Micro-average quality numbers:\")\n",
    "pd.DataFrame(predictions_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8837702390131073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Y_count</th>\n",
       "      <th>Pred_count</th>\n",
       "      <th>Error</th>\n",
       "      <th>Accuarcy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPD</td>\n",
       "      <td>344</td>\n",
       "      <td>0</td>\n",
       "      <td>0.132614</td>\n",
       "      <td>0.867386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CB</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122205</td>\n",
       "      <td>0.877795</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EWC</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089052</td>\n",
       "      <td>0.910948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exec</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.863917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FWE</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072089</td>\n",
       "      <td>0.927911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SP</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097147</td>\n",
       "      <td>0.902853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RE</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.078643</td>\n",
       "      <td>0.921357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sup</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0.099460</td>\n",
       "      <td>0.900540</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SW</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152660</td>\n",
       "      <td>0.847340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TEPE</td>\n",
       "      <td>605</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233231</td>\n",
       "      <td>0.766769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VMG</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140324</td>\n",
       "      <td>0.859676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OTH</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041249</td>\n",
       "      <td>0.958751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Y_count  Pred_count     Error  Accuarcy  Precision  Recall\n",
       "0    CPD      344           0  0.132614  0.867386        0.0     0.0\n",
       "1     CB      317           0  0.122205  0.877795        0.0     0.0\n",
       "2    EWC      231           0  0.089052  0.910948        0.0     0.0\n",
       "3   Exec      353           0  0.136083  0.863917        0.0     0.0\n",
       "4    FWE      187           0  0.072089  0.927911        0.0     0.0\n",
       "5     SP      252           0  0.097147  0.902853        0.0     0.0\n",
       "6     RE      204           0  0.078643  0.921357        0.0     0.0\n",
       "7    Sup      258           0  0.099460  0.900540        0.0     0.0\n",
       "8     SW      396           0  0.152660  0.847340        0.0     0.0\n",
       "9   TEPE      605           0  0.233231  0.766769        0.0     0.0\n",
       "10   VMG      364           0  0.140324  0.859676        0.0     0.0\n",
       "11   OTH      107           0  0.041249  0.958751        0.0     0.0"
      ]
     },
     "execution_count": 1172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESULTS PER LABEL\n",
    "# Last's year function\n",
    "def theme_results(Ytrue, Ypred):\n",
    "    '''Calculate accuracies for theme classification\n",
    "    Parameters\n",
    "    ----------\n",
    "    Ytrue : array of shape (n_obeservations, n_labels)\n",
    "        Correct labels for the 12 text classifications\n",
    "    Ypred : array of shape (n_obeservations, n_labels)\n",
    "        Predicted labels for the 12 text classifications\n",
    "    Returns\n",
    "    -------\n",
    "    overall_results : dataframes of overall evaluation metrics\n",
    "    theme_results : dataframe of evaluation metrics by class\n",
    "    '''\n",
    "    # Calculate individual accuracies and evaluation metrics for each class\n",
    "    labels = ['CPD', 'CB', 'EWC', 'Exec', 'FWE', 'SP', 'RE', 'Sup', 'SW',\n",
    "              'TEPE', 'VMG', 'OTH']\n",
    "    Y_count = []\n",
    "    pred_count = []\n",
    "    error = []\n",
    "    #dummy_diff = []\n",
    "    accuracies = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in np.arange(Ytrue.shape[1]):\n",
    "        Y_count.append(np.sum(Ytrue[:, i] == 1))\n",
    "        pred_count.append(np.sum(Ypred[:, i] == 1))\n",
    "        error.append(1 - accuracy_score(Ytrue[:, i], Ypred[:, i]))\n",
    "        #dummy_diff.append((np.mean(Ytrue[:, i] == 1)) - error[i])\n",
    "        accuracies.append(accuracy_score(Ytrue[:, i], Ypred[:, i]))\n",
    "        precision.append(precision_score(Ytrue[:, i], Ypred[:, i]))\n",
    "        recall.append(recall_score(Ytrue[:, i], Ypred[:, i]))\n",
    "    theme_results = pd.DataFrame({'Label': labels,\n",
    "                                  'Y_count': Y_count,\n",
    "                                  'Pred_count': pred_count,\n",
    "                                  'Error': error,\n",
    "                                 # 'Dummy_Diff': dummy_diff,\n",
    "                                  'Accuarcy': accuracies,\n",
    "                                  'Precision': precision,\n",
    "                                  'Recall': recall})\n",
    "    return theme_results\n",
    "\n",
    "\n",
    "print(np.mean(theme_results(y_valid, y_pred_binary)['Accuarcy']))\n",
    "theme_results(y_valid, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
