{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove stop words and punctuation (done)\n",
    "# Step 2: Check if text has line breaks and remove those (done)\n",
    "# Step 3: Separate words like I'm to I am (done)\n",
    "# Step 4: Remove names, dates, etc. (named entities)\n",
    "# Step 5: Tokenize the text (done)\n",
    "# Step 6: Change the text to lower case (done)\n",
    "# Step 7: Lemmatize the words (done)\n",
    "\n",
    "# REMOVE PERSON, LOC, ORG, GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from `preprocess` function of lab 3 \n",
    "# of this course (575 Advance Learning Machine)\n",
    "\n",
    "def preprocess_comments(text, \n",
    "               min_token_len = 2, \n",
    "               irrelevant_pos = [#'PROPN', # erase proper names as 'George', but also words as orange or apple\n",
    "                                 'PRON',\n",
    "                                 'SPACE',\n",
    "                                 'PUNCT', # as:  . , ; ... \" ? ! $\n",
    "#                                  'DET', # commented since it removes 'no' which changes meaning of a sentence\n",
    "                                 'ADV',   # removes words including 'why'\n",
    "                                 'ADP',\n",
    "                                 'CCONJ'\n",
    "               ], avoid_entities = ['PERSON', 'ORG', 'LOC', 'GPE']): \n",
    "    \"\"\"\n",
    "    Given text, min_token_len, irrelevant_pos and avoid_entities, carries out \n",
    "    preprocessing of the text and returns list of preprocessed text. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (list) \n",
    "        the list of text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    avoid_entities : (list)\n",
    "        a list of entity labels to be avoided\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (list) list of preprocessed text\n",
    "    \n",
    "    Example\n",
    "    -------------\n",
    "    >>> example = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]\n",
    "\n",
    "    >>> preprocess(example)\n",
    "    (output:) ['hello be love swimming', 'be good cook', 'contact']\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    others = [\"'s\"]\n",
    "    \n",
    "    for sent in text:\n",
    "        \n",
    "        sent = sent.lower()\n",
    "        sent = re.sub(r\"facebook\", \"social media\", sent)\n",
    "        sent = re.sub(r\"twitter\", \"social media\", sent)\n",
    "        sent = re.sub(r\"instagram\", \"social media\", sent)\n",
    "        sent = re.sub(r\"whatsapp\", \"social media\", sent)\n",
    "        sent = re.sub(r\"linkedin\", \"social media\", sent)\n",
    "        sent = re.sub(r\"snapchat\", \"social media\", sent)\n",
    "        \n",
    "        result_sent = []\n",
    "        \n",
    "        doc = nlp(sent)\n",
    "        entities = [str(ent) for ent in doc.ents if ent.label_ in avoid_entities] # This helps to detect names of persons, of organization and dates\n",
    "        \n",
    "        for token in doc:\n",
    "#             print(token.pos_)\n",
    "            \n",
    "            if (token.like_email or\n",
    "                token.like_url or\n",
    "                token.pos_ in irrelevant_pos or\n",
    "                str(token) in entities or       # removes names entities\n",
    "                str(token.lemma_) in others or\n",
    "                len(token) < min_token_len):\n",
    "                continue\n",
    "            else:\n",
    "                result_sent.append(token.lemma_) # carries out lemmatization\n",
    "    \n",
    "        result.append(\" \".join(result_sent))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Why I got this information from Facebook today\", \"News from Whatsapp isn't reliable\",\n",
    "          \"Do you eat apples?\", \"Apple is a profitable company\", \n",
    "          \"Twitter is a great source of information!!!\",\n",
    "          \"Victor's computer's are great!!\", \"Isn't it a great day, Bob? I loved the movie and spending time with you.\", \n",
    "          \"The sky is not always blue underneath. Remember that.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get this information social medium today',\n",
       " 'news social medium be not reliable',\n",
       " 'do eat apple',\n",
       " 'be profitable company',\n",
       " 'social media be great source information',\n",
       " 'computer be great',\n",
       " 'be not great day love the movie spending time',\n",
       " 'the be not blue remember that']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_comments(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello be love swimming', 'be good cook', 'contact']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_comments(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
