{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove stop words and punctuation (done)\n",
    "# Step 2: Check if text has line breaks and remove those (done)\n",
    "# Step 3: Separate words like I'm to I am (done)\n",
    "# Step 4: Remove names, dates, etc. (named entities)\n",
    "# Step 5: Tokenize the text (done)\n",
    "# Step 6: Change the text to lower case (done)\n",
    "# Step 7: Lemmatize the words (done)\n",
    "\n",
    "# REMOVE PERSON, LOC, ORG, GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function adapted from `preprocess` function shared by Varada\n",
    "# in course 575 Advance Learning Machine\n",
    "\n",
    "def preprocess_comments(text, \n",
    "                        min_token_len = 2, \n",
    "                        irrelevant_pos = ['PRON', 'SPACE', 'PUNCT', 'ADV', \n",
    "                                          'ADP', 'CCONJ', 'AUX', 'PRP'],\n",
    "                        avoid_entities = ['PERSON', 'ORG', 'LOC', 'GPE']):\n",
    "# note: Didn't use the following options in the `preprocess_comments`...\n",
    "#    - 'PROPN' because it erases proper names as 'George', but also words as orange.\n",
    "#    - 'DET' since it removes the word 'no', which changes the meaning of a sentence.\n",
    "# *for more information see link: https://universaldependencies.org/u/pos/\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, irrelevant_pos and avoid_entities, carries out \n",
    "    preprocessing of the text and returns list of preprocessed text. \n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    text : (list) \n",
    "        the list of text to be preprocessed\n",
    "    min_token_len : (int) \n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list) \n",
    "        a list of irrelevant pos tags\n",
    "    avoid_entities : (list)\n",
    "        a list of entity labels to be avoided\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    (list) list of preprocessed text\n",
    "    \n",
    "    Example\n",
    "    -------------\n",
    "    >>> example = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]\n",
    "\n",
    "    >>> preprocess(example)\n",
    "    (output:) ['hello love swimming', 'good cook', 'contact']\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    others = [\"'s\", \"the\", \"that\", \"this\", \"to\", \"-PRON-\"]\n",
    "    # I add \"-PRON-\" that erase \"my\", \"your\", etc. other way to erase them is to\n",
    "    #   use adding 'DET' to irrelevant_pos but it would erase the word 'no' too.\n",
    "    \n",
    "    for sent in text:\n",
    "        sent = sent.lower()\n",
    "        sent = re.sub(r\"facebook\", \"social media\", sent)\n",
    "        sent = re.sub(r\"twitter\", \"social media\", sent)\n",
    "        sent = re.sub(r\"instagram\", \"social media\", sent)\n",
    "        sent = re.sub(r\"whatsapp\", \"social media\", sent)\n",
    "        sent = re.sub(r\"linkedin\", \"social media\", sent)\n",
    "        sent = re.sub(r\"snapchat\", \"social media\", sent)\n",
    "        \n",
    "        result_sent = []\n",
    "        doc = nlp(sent)\n",
    "        entities = [str(ent) for ent in doc.ents if ent.label_ in avoid_entities]\n",
    "        # This helps to detect names of persons, organization and dates\n",
    "        \n",
    "        for token in doc:            \n",
    "            if (token.like_email or\n",
    "                token.like_url or\n",
    "                token.pos_ in irrelevant_pos or\n",
    "                str(token) in entities or\n",
    "                str(token.lemma_) in others or\n",
    "                len(token) < min_token_len):\n",
    "                continue\n",
    "            else:\n",
    "                result_sent.append(token.lemma_)\n",
    "        result.append(\" \".join(result_sent))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_1 = [\"Why I got this information from Facebook today?\", \"News from Whatsapp isn't reliable\",\n",
    "          \"Do you eat apples?\", \"Apple is a profitable company\", \n",
    "          \"Twitter is a great source of information!!!\",\n",
    "          \"Sukriti, Carlina, Victor and Karan, are a great team!\", \"Steve's computer's are great!!\",\n",
    "          \"Isn't it a great day, Bob? I loved the movie and spending time with you.\", \n",
    "          \"The sky is not always blue underneath. Remember that.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get information social medium today',\n",
       " 'news social medium not reliable',\n",
       " 'eat apple',\n",
       " 'profitable company',\n",
       " 'social media great source information',\n",
       " 'great team',\n",
       " 'computer great',\n",
       " 'not great day love movie spending time',\n",
       " 'not blue remember']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_comments(example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_2 = [\"Hello, I'm George and I love swimming!\",\n",
    "                   \"I am a really good cook; what about you?\",\n",
    "                   \"Contact me at george23@gmail.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello love swimming', 'good cook', 'contact']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_comments(example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess BC Stats datasets\n",
    "*This code preprocess the \"Comments\" who have sensible data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The next code code consider that your datasets \n",
    "#   are located in the \"../data/\" directory.\n",
    "\n",
    "##################\n",
    "### Question 1 ###\n",
    "##################\n",
    "\n",
    "# train dataset\n",
    "X_train = pd.read_csv(\"../data/X_train.csv\")\n",
    "X_train_pp = preprocess_comments(X_train['Comment'])\n",
    "\n",
    "# validation dataset\n",
    "X_valid = pd.read_csv(\"../data/X_valid.csv\")\n",
    "X_valid_pp = preprocess_comments(X_valid['Comment'])\n",
    "\n",
    "\n",
    "##################\n",
    "### Question 2 ###\n",
    "##################\n",
    "\n",
    "### Supervised:\n",
    "\n",
    "# train dataset\n",
    "X_train_q2 = pd.read_csv(\"../data/X_train_q2.csv\")\n",
    "X_train_q2_pp = preprocess_comments(X_train_q2['Comment'])\n",
    "\n",
    "# validation dataset\n",
    "X_valid_q2 = pd.read_csv(\"../data/X_valid_q2.csv\")\n",
    "X_valid_q2_pp = preprocess_comments(X_valid_q2['Comment'])\n",
    "\n",
    "\n",
    "### Unsupervised:\n",
    "unsuperv_q2 = pd.read_csv(\"../data/unsuperv_q2.csv\")\n",
    "unsuperv_q2_pp = preprocess_comments(X_train_q2['Comment'])\n",
    "\n",
    "####################################\n",
    "### Saving Preprocessed datasets ###\n",
    "####################################\n",
    "\n",
    "pd.DataFrame(X_train_pp, columns=['Comment']).to_csv('../data/X_train_pp.csv', index=False)\n",
    "pd.DataFrame(X_valid_pp, columns=['Comment']).to_csv('../data/X_valid_pp.csv', index=False)\n",
    "pd.DataFrame(X_train_q2_pp, columns=['Comment']).to_csv('../data/X_train_q2_pp.csv', index=False)\n",
    "pd.DataFrame(X_valid_q2_pp, columns=['Comment']).to_csv('../data/X_valid_q2_pp.csv', index=False)\n",
    "pd.DataFrame(unsuperv_q2_pp, columns=['Comment']).to_csv('../data/unsuperv_q2_pp.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
